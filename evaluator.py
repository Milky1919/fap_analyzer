# -*- coding: utf-8 -*-
import sqlite3
import datetime
import json
import pickle
import logging
import re
import unicodedata
import argparse
import traceback
from typing import List, Dict, Any, Optional, Set

import MeCab
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# å…±æœ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import db_utils
import config

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾© ---

class Post:
    """
    postsãƒ†ãƒ¼ãƒ–ãƒ«ã®1ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    é–¢é€£ã™ã‚‹ã‚¿ã‚°ã¯DBã‹ã‚‰JOINã—ã¦å–å¾—ã™ã‚‹ã€‚
    """
    def __init__(self, conn: sqlite3.Connection, db_row: Dict[str, Any]):
        # åŸºæœ¬æƒ…å ±ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.post_id: int = db_row['post_id']
        dt_str = db_row.get('post_datetime')
        self.post_datetime: Optional[datetime.datetime] = datetime.datetime.strptime(dt_str, '%Y/%m/%d %H:%M') if dt_str else None
        self.title: str = db_row['title']
        self.purpose: str = db_row['purpose']
        self.original_text: str = db_row['original_text']
        self.author_name: str = db_row['author_name']
        age_str = (db_row['author_real_age'] or '0').replace('ä»£', '').replace('ï¼Ÿ', '0')
        self.author_real_age: int = int(age_str) if age_str.isdigit() else 0
        self.author_real_gender: str = db_row['author_real_gender']
        self.author_char_race: str = db_row['author_char_race']
        self.author_char_gender: str = db_row['author_char_gender']
        self.author_char_job: str = db_row['author_char_job']
        self.server: str = db_row['server']
        self.voice_chat: str = db_row['voice_chat']
        self.server_transfer: str = db_row['server_transfer']
        self.sub_char_ok: bool = bool(db_row['sub_char_ok'])

        # ã‚¿ã‚°æƒ…å ±ã‚’DBã‹ã‚‰å–å¾—
        self.tags_by_category: Dict[str, List[str]] = self._load_tags(conn)

        # å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆ
        self.all_tags: Set[str] = self._create_all_tags_set()

    def _load_tags(self, conn: sqlite3.Connection) -> Dict[str, List[str]]:
        """post_idã«ç´ã¥ãã‚¿ã‚°ã‚’DBã‹ã‚‰å–å¾—ã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ã—ã¦è¿”ã™"""
        cursor = conn.cursor()
        query = """
            SELECT t.tag_category, t.tag_name
            FROM post_tags pt
            JOIN tags t ON pt.tag_id = t.tag_id
            WHERE pt.post_id = ?
        """
        cursor.execute(query, (self.post_id,))
        tags = {}
        for row in cursor.fetchall():
            category, name = row['tag_category'], row['tag_name']
            if category not in tags:
                tags[category] = []
            tags[category].append(name)
        return tags

    def _create_all_tags_set(self) -> Set[str]:
        """è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ã®å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆã™ã‚‹"""
        tags = set()
        for category_tags in self.tags_by_category.values():
            tags.update(category_tags)

        # ã‚¿ã‚°ã¨ã—ã¦æ‰±ã†ä»–ã®å±æ€§ã‚‚è¿½åŠ 
        if self.voice_chat:
            tags.add(f"VC:{self.voice_chat}")
        if self.server_transfer:
            tags.add(self.server_transfer)
        if self.sub_char_ok:
            tags.add("ã‚µãƒ–ã‚­ãƒ£ãƒ©å¯")
        return tags

    def __repr__(self) -> str:
        return f"<Post id={self.post_id} name='{self.author_name}'>"

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ (è©•ä¾¡å¯¾è±¡å–å¾—) ---

def get_posts_to_evaluate(conn: sqlite3.Connection, args: argparse.Namespace) -> List[Post]:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«åŸºã¥ã„ã¦è©•ä¾¡å¯¾è±¡ã®æŠ•ç¨¿ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
    cursor = conn.cursor()
    if args.post_id:
        logging.info(f"--post-idãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚æŠ•ç¨¿ID: {args.post_id} ã‚’è©•ä¾¡ã—ã¾ã™ã€‚")
        cursor.execute("SELECT * FROM posts WHERE post_id = ?", (args.post_id,))
    elif args.re_evaluate_all:
        logging.info("--re-evaluate-allãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚å…¨æŠ•ç¨¿ã‚’å†è©•ä¾¡ã—ã¾ã™ã€‚")
        cursor.execute("DELETE FROM evaluation_scores")
        conn.commit()
        logging.info("æ—¢å­˜ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        cursor.execute("SELECT * FROM posts ORDER BY post_datetime ASC")
    else:
        logging.info("æœªè©•ä¾¡ã®æŠ•ç¨¿ã‚’ã™ã¹ã¦è©•ä¾¡ã—ã¾ã™ã€‚ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰")
        cursor.execute("""
            SELECT p.* FROM posts p
            LEFT JOIN evaluation_scores es ON p.post_id = es.post_id
            WHERE es.post_id IS NULL
            ORDER BY p.post_datetime ASC
        """)
    rows = cursor.fetchall()
    if not rows:
        logging.info("è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    # Postã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç”Ÿæˆã«ã¯DBæ¥ç¶šãŒå¿…è¦
    target_posts = [Post(conn, dict(row)) for row in rows]
    logging.info(f"{len(target_posts)} ä»¶ã®æŠ•ç¨¿ã‚’è©•ä¾¡å¯¾è±¡ã¨ã—ã¦å–å¾—ã—ã¾ã—ãŸã€‚")
    return target_posts

def get_all_original_texts(conn: sqlite3.Connection) -> List[str]:
    """postsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨ã¦ã®original_textã‚’å–å¾—ã™ã‚‹"""
    cursor = conn.cursor()
    cursor.execute("SELECT original_text FROM posts ORDER BY post_datetime ASC")
    return [row['original_text'] for row in cursor.fetchall() if row['original_text']]

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢é€£ ---
def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹"""
    if not text: return ""
    text = unicodedata.normalize('NFKC', text)
    text = text.lower()
    text = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ]', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def tokenize_with_mecab(text: str) -> str:
    """MeCabã§ã‚ã‹ã¡æ›¸ã"""
    tagger = MeCab.Tagger("-Owakati -r /etc/mecabrc")
    return tagger.parse(text).strip()

def prepare_vectorizer(conn: sqlite3.Connection) -> Optional[TfidfVectorizer]:
    """TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’æº–å‚™"""
    try:
        with open(config.VECTORIZER_PATH, 'rb') as f:
            vectorizer = pickle.load(f)
        logging.info(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{config.VECTORIZER_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return vectorizer
    except FileNotFoundError:
        logging.info("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")
        corpus = get_all_original_texts(conn)
        if not corpus:
            logging.error("ã‚³ãƒ¼ãƒ‘ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return None
        normalized_corpus = [normalize_text(text) for text in corpus]
        stop_words = ['ã§ã™', 'ã¾ã™', 'ã®', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¨', 'ã‚‚', 'ã§', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹']
        vectorizer = TfidfVectorizer(tokenizer=tokenize_with_mecab, stop_words=stop_words)
        vectorizer.fit(normalized_corpus)
        with open(config.VECTORIZER_PATH, 'wb') as f:
            pickle.dump(vectorizer, f)
        logging.info(f"å­¦ç¿’æ¸ˆã¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{config.VECTORIZER_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return vectorizer
    except Exception as e:
        logging.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ ---

def get_candidate_posts(conn: sqlite3.Connection, target_post: Post) -> List[Post]:
    """é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§å€™è£œæŠ•ç¨¿ã‚’å–å¾—"""
    cursor = conn.cursor()
    try:
        query = """
            SELECT * FROM posts
            WHERE author_real_gender = ? AND author_real_age = ? AND author_char_race = ?
              AND author_char_gender = ? AND post_id != ? AND post_datetime < ?
        """
        params = (
            target_post.author_real_gender,
            f"{target_post.author_real_age}ä»£" if target_post.author_real_age != 0 else "ï¼Ÿä»£",
            target_post.author_char_race,
            target_post.author_char_gender,
            target_post.post_id,
            target_post.post_datetime.strftime('%Y/%m/%d %H:%M') if target_post.post_datetime else ''
        )
        cursor.execute(query, params)
        rows = cursor.fetchall()
        candidate_posts = [Post(conn, dict(row)) for row in rows]
        logging.info(f"é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Š {len(candidate_posts)} ä»¶ã®æ¯”è¼ƒå€™è£œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        return candidate_posts
    except sqlite3.Error as e:
        logging.error(f"å€™è£œæŠ•ç¨¿ã®å–å¾—ä¸­ã«DBã‚¨ãƒ©ãƒ¼: {e}")
        return []

def save_evaluation_score(conn: sqlite3.Connection, post_id: int, result: Dict[str, Any]):
    """è©•ä¾¡çµæœã‚’DBã«ä¿å­˜"""
    cursor = conn.cursor()
    try:
        query = """
            INSERT OR REPLACE INTO evaluation_scores (
                post_id, evaluation_datetime, unique_score, most_similar_post_id,
                max_similarity_score, is_repost, penalty, score_breakdown, author_post_count
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
        """
        params = (
            post_id, datetime.datetime.now().isoformat(), result.get('unique_score'),
            result.get('most_similar_post_id'), result.get('max_similarity_score'),
            result.get('is_repost'), result.get('penalty'),
            json.dumps(result.get('score_breakdown', {}), ensure_ascii=False),
            result.get('author_post_count')
        )
        cursor.execute(query, params)
        conn.commit()
        logging.info(f"Post ID {post_id} ã®è©•ä¾¡çµæœã‚’DBã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except sqlite3.Error as e:
        logging.error(f"è©•ä¾¡çµæœã®ä¿å­˜ä¸­ã«DBã‚¨ãƒ©ãƒ¼: {e}")

def print_summary(target_post: Post, result: Dict[str, Any]):
    """è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›"""
    print("\n" + "="*60 + f"\n{'è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼':^60}\n" + "="*60)
    print(f" è©•ä¾¡å¯¾è±¡æŠ•ç¨¿ID: {target_post.post_id}")
    print(f" æŠ•ç¨¿è€…å:         {target_post.author_name}")
    print(f" æŠ•ç¨¿æ—¥æ™‚:         {target_post.post_datetime.strftime('%Y-%m-%d %H:%M') if target_post.post_datetime else 'N/A'}")
    print("-"*60)
    print(f" ã€æœ€çµ‚ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã€‘: {result['unique_score']:.2f} / 100")
    print("-"*60)
    print(f"   - æœ€å¤§é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢:     {result['max_similarity_score']:.2f}")
    print(f"   - æœ€ã‚‚é¡ä¼¼ã—ãŸæŠ•ç¨¿ID: {result.get('most_similar_post_id', 'N/A')}")
    print(f"   - å†æŠ•ç¨¿åˆ¤å®š:           {'ã¯ã„' if result['is_repost'] else 'ã„ã„ãˆ'}")
    print(f"   - é©ç”¨ãƒšãƒŠãƒ«ãƒ†ã‚£:       {result['penalty']:.2f}")
    print("="*60 + "\n")

def calculate_static_profile_score(post_a: Post, post_b: Post) -> float:
    """A) é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢"""
    score = 0
    age_diff = abs(post_a.author_real_age - post_b.author_real_age)
    score += config.REAL_AGE_SCORE_MAX * (1 - min(5, age_diff) / 5)
    if post_a.author_char_race == post_b.author_char_race and post_a.author_char_gender == post_b.author_char_gender:
        score += config.CHAR_INFO_SCORE_MAX
    if post_a.author_char_job == post_b.author_char_job and post_a.author_name == post_b.author_name:
        score += config.VARIABLE_PROFILE_SCORE_MAX
    return score

def calculate_behavioral_pattern_score(post_a: Post, post_b: Post, tag_weights: Dict[str, float]) -> float:
    """B) è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢"""
    intersection = post_a.all_tags.intersection(post_b.all_tags)
    union = post_a.all_tags.union(post_b.all_tags)
    if not union: return 0.0
    intersection_weight = sum(tag_weights.get(tag, 0) for tag in intersection)
    union_weight = sum(tag_weights.get(tag, 0) for tag in union)
    return config.BEHAVIORAL_PATTERN_SCORE_MAX * (intersection_weight / union_weight) if union_weight > 0 else 0.0

def _calculate_tag_weights_from_db(conn: sqlite3.Connection) -> Dict[str, float]:
    """DBã‹ã‚‰å…¨ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’å…ƒã«ã€ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼‰"""
    cursor = conn.cursor()
    tag_counts = {}

    # 1. 'tags'ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç”±æ¥ã™ã‚‹ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’åŠ¹ç‡çš„ã«ä¸€æ‹¬ã§è¨ˆç®—
    query = """
        SELECT t.tag_name, COUNT(pt.post_id) as count
        FROM tags t
        JOIN post_tags pt ON t.tag_id = pt.tag_id
        GROUP BY t.tag_name;
    """
    cursor.execute(query)
    for row in cursor.fetchall():
        tag_counts[row['tag_name']] = row['count']

    # 2. 'posts'ãƒ†ãƒ¼ãƒ–ãƒ«ã®å±æ€§ã«ç”±æ¥ã™ã‚‹ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’è¨ˆç®—
    # (voice_chat, server_transfer, sub_char_ok)
    cursor.execute("SELECT voice_chat, server_transfer, sub_char_ok FROM posts")
    posts_attributes = cursor.fetchall()

    for row in posts_attributes:
        # Voice Chat
        if row['voice_chat']:
            vc_tag = f"VC:{row['voice_chat']}"
            tag_counts[vc_tag] = tag_counts.get(vc_tag, 0) + 1
        # Server Transfer
        if row['server_transfer']:
            st_tag = row['server_transfer']
            tag_counts[st_tag] = tag_counts.get(st_tag, 0) + 1
        # Sub Char OK
        if bool(row['sub_char_ok']):
            sc_tag = "ã‚µãƒ–ã‚­ãƒ£ãƒ©å¯"
            tag_counts[sc_tag] = tag_counts.get(sc_tag, 0) + 1

    # 3. é‡ã¿ã‚’è¨ˆç®—: w(t) = 1 / np.log(1 + (å‡ºç¾å›æ•°))
    tag_weights = {tag: 1 / np.log(1 + count) for tag, count in tag_counts.items() if count > 0}
    logging.info(f"{len(tag_weights)}å€‹ã®ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚")
    return tag_weights

def prepare_tag_weights(conn: sqlite3.Connection) -> Dict[str, float]:
    """ã‚¿ã‚°ã®é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    try:
        with open(config.TAG_WEIGHTS_PATH, 'rb') as f:
            tag_weights = pickle.load(f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{config.TAG_WEIGHTS_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return tag_weights
    except FileNotFoundError:
        logging.info("ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")
        tag_weights = _calculate_tag_weights_from_db(conn)
        with open(config.TAG_WEIGHTS_PATH, 'wb') as f:
            pickle.dump(tag_weights, f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{config.TAG_WEIGHTS_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return tag_weights
    except Exception as e:
        logging.error(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def create_stylistic_feature_vector(text: str) -> np.ndarray:
    """æ–‡ä½“ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
    if not text: return np.zeros(8)
    text_len = len(text)
    features = [
        len(re.findall(r'[ï¼-ï½]', text)) / (len(re.findall(r'[!-~]', text)) + len(re.findall(r'[ï¼-ï½]', text)) or 1),
        len(re.findall(r'[ï¼ï¼Ÿwâ™ªâœ¨ğŸ€ğŸ€ğŸ°ğŸ§¸ğŸ’Œ]', text)) / text_len,
        len(re.findall(r'([ã€‚ã€])\s', text)) / (len(re.findall(r'[ã€‚ã€]', text)) or 1),
        text.count('\n') / text_len,
        len(re.findall(r'[\u3040-\u309F]', text)) / text_len,
        len(re.findall(r'[\u30A0-\u30FF]', text)) / text_len,
        len(re.findall(r'[\u4E00-\u9FFF]', text)) / text_len,
        0.0
    ]
    return np.array(features)

def calculate_linguistic_fingerprint_score(post_a: Post, post_b: Post, vectorizer: TfidfVectorizer) -> Dict[str, float]:
    """C) è¨€èªçš„æŒ‡ç´‹é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢"""
    texts = [normalize_text(post_a.original_text), normalize_text(post_b.original_text)]
    tfidf_vectors = vectorizer.transform(texts)
    semantic_sim = cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])[0][0]
    semantic_score = config.SEMANTIC_SCORE_MAX * semantic_sim
    style_vec_a = create_stylistic_feature_vector(post_a.original_text)
    style_vec_b = create_stylistic_feature_vector(post_b.original_text)
    euclidean_dist = np.linalg.norm(style_vec_a - style_vec_b)
    stylistic_sim = np.exp(-0.5 * euclidean_dist)
    stylistic_score = config.STYLISTIC_SCORE_MAX * stylistic_sim
    reliability = min(1.0, len(post_a.original_text) / config.RELIABILITY_TEXT_LENGTH if post_a.original_text else 0.0)
    final_score = (semantic_score + stylistic_score) * reliability
    return {"total": final_score, "semantic": semantic_score, "stylistic": stylistic_score}

def trace_repost_chain(conn: sqlite3.Connection, start_post_id: int, visited: Set[int]) -> int:
    """å†æŠ•ç¨¿ã®é€£é–ã‚’é¡ã‚‹"""
    if start_post_id is None or start_post_id in visited: return 0
    visited.add(start_post_id)
    cursor = conn.cursor()
    cursor.execute("SELECT most_similar_post_id, is_repost FROM evaluation_scores WHERE post_id = ?", (start_post_id,))
    result = cursor.fetchone()
    if not result or not result['is_repost']: return 1
    return 1 + trace_repost_chain(conn, result['most_similar_post_id'], visited)

def get_post_by_id(conn: sqlite3.Connection, post_id: int) -> Optional[Post]:
    """æŒ‡å®šã•ã‚ŒãŸpost_idã®Postã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã™ã‚‹"""
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM posts WHERE post_id = ?", (post_id,))
    row = cursor.fetchone()
    return Post(conn, dict(row)) if row else None

def evaluate_post(target_post: Post, candidates: List[Post], vectorizer: TfidfVectorizer, tag_weights: Dict[str, float], conn: sqlite3.Connection) -> Dict[str, Any]:
    """1ä»¶ã®æŠ•ç¨¿ã‚’è©•ä¾¡ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ãƒ­ã‚¸ãƒƒã‚¯æ”¹å–„ç‰ˆï¼‰"""
    if not candidates:
        return {"unique_score": 100.0, "most_similar_post_id": None, "max_similarity_score": 0.0,
                "is_repost": 0, "penalty": 0.0, "author_post_count": 1, "score_breakdown": {"reason": "No candidates"}}

    max_sim_score, most_sim_post_id, best_scores = 0.0, None, {}
    most_similar_post_obj = None # çŒ¶äºˆæœŸé–“ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã«Postã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè‡ªä½“ã‚‚ä¿æŒ

    for cand_post in candidates:
        static = calculate_static_profile_score(target_post, cand_post)
        behavioral = calculate_behavioral_pattern_score(target_post, cand_post, tag_weights)
        linguistic = calculate_linguistic_fingerprint_score(target_post, cand_post, vectorizer)
        base_sim = static + behavioral + linguistic["total"]
        bonus = config.CONSISTENCY_BONUS if target_post.server == cand_post.server else 0
        current_sim = base_sim + bonus
        if current_sim > max_sim_score:
            max_sim_score = current_sim
            most_sim_post_id = cand_post.post_id
            most_similar_post_obj = cand_post # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ›´æ–°
            best_scores = {"static": static, "behavioral": behavioral, **linguistic, "bonus": bonus}

    is_repost = 1 if max_sim_score >= config.REPOST_THRESHOLD and best_scores.get("static", 0) >= config.REPOST_STATIC_SCORE_THRESHOLD else 0
    post_count = 1 + trace_repost_chain(conn, most_sim_post_id, set()) if is_repost and most_sim_post_id else 1
    penalty = 0.0

    if is_repost and target_post.post_datetime and most_similar_post_obj and most_similar_post_obj.post_datetime:
        # 1. çŒ¶äºˆæœŸé–“ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ã®ãƒã‚§ãƒƒã‚¯
        time_diff = target_post.post_datetime - most_similar_post_obj.post_datetime
        if time_diff.total_seconds() / 60 < config.REPOST_GRACE_PERIOD_MINUTES:
            penalty = 0.0
            logging.info(f"å†æŠ•ç¨¿çŒ¶äºˆæœŸé–“å†…ã®ãŸã‚ã€ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ 0 ã«è¨­å®šã—ã¾ã™ã€‚(æ™‚é–“å·®: {time_diff})")
        else:
            # 2. æ–°ã—ã„ãƒšãƒŠãƒ«ãƒ†ã‚£å›å¾©æ›²ç·šã®è¨ˆç®—
            N = post_count
            # Dã¯ã€Œé¡ä¼¼æŠ•ç¨¿ã‹ã‚‰ã®ã€çµŒéæ—¥æ•°ã§ã¯ãªãã€ã€Œè©•ä¾¡å®Ÿè¡Œæ™‚ç‚¹ã‹ã‚‰ã®ã€çµŒéæ—¥æ•°
            D = (datetime.datetime.now() - target_post.post_datetime).days

            base_penalty = config.BASE_PENALTY_COEFFICIENT - (config.PENALTY_PER_REPOST * (N - 1))

            recovery = 0.0
            if D > config.NO_RECOVERY_PERIOD_DAYS:
                # å›å¾©æœŸé–“ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã®ã¿å›å¾©é‡ã‚’è¨ˆç®—
                recovery_days = D - config.NO_RECOVERY_PERIOD_DAYS
                recovery_period = config.DAYS_FOR_FULL_RECOVERY - config.NO_RECOVERY_PERIOD_DAYS
                recovery_rate = min(1.0, recovery_days / recovery_period) if recovery_period > 0 else 1.0

                recovery_limit = max(config.MIN_RECOVERY_LIMIT, config.MAX_RECOVERY_LIMIT - (N - 2))
                recovery = recovery_rate * recovery_limit

            penalty = base_penalty + recovery

    unique_score = max(0, min(100, 100 - max_sim_score + penalty))
    return {"unique_score": unique_score, "most_similar_post_id": most_sim_post_id, "max_similarity_score": max_sim_score,
            "is_repost": is_repost, "penalty": penalty, "author_post_count": post_count, "score_breakdown": best_scores}

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    """è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="æŠ•ç¨¿ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    parser.add_argument('--post-id', type=int, help='æŒ‡å®šã—ãŸpost_idã®æŠ•ç¨¿ã®ã¿ã‚’è©•ä¾¡ã™ã‚‹')
    parser.add_argument('--re-evaluate-all', action='store_true', help='ã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å¼·åˆ¶çš„ã«å†è©•ä¾¡ã™ã‚‹')
    args = parser.parse_args()
    logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é–‹å§‹ã—ã¾ã™ ---")
    conn = None
    try:
        conn = db_utils.setup_database(config.DB_NAME)
        conn.row_factory = sqlite3.Row
        # è©•ä¾¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ è¿½åŠ ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚‚setup_databaseã«å«ã‚ã‚‹ã¹ãã ãŒã€
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€å…ƒã®evaluator.pyã«ã‚ã£ãŸå‡¦ç†ã‚’ã“ã“ã«æ®‹ã™
        setup_evaluation_specific_tables(conn)

        target_posts = get_posts_to_evaluate(conn, args)
        if not target_posts:
            logging.info("è©•ä¾¡å¯¾è±¡ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return
        vectorizer = prepare_vectorizer(conn)
        if not vectorizer:
            logging.error("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return
        tag_weights = prepare_tag_weights(conn)

        for i, target_post in enumerate(target_posts):
            logging.info(f"--- ({i+1}/{len(target_posts)}) post_id: {target_post.post_id} ã®è©•ä¾¡ã‚’é–‹å§‹ ---")
            candidates = get_candidate_posts(conn, target_post)
            result = evaluate_post(target_post, candidates, vectorizer, tag_weights, conn)
            save_evaluation_score(conn, target_post.post_id, result)
            print_summary(target_post, result)
    except Exception as e:
        logging.critical("è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", exc_info=True)
        if conn:
            db_utils.log_to_db(conn, 'CRITICAL', 'evaluator.py', f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}", traceback.format_exc())
    finally:
        if conn:
            conn.close()
        logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ ---")

def setup_evaluation_specific_tables(conn: sqlite3.Connection):
    """
    evaluator.pyå›ºæœ‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«è¨­å®šï¼ˆevaluation_scoresãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ è¿½åŠ ãªã©ï¼‰
    """
    cursor = conn.cursor()
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS evaluation_scores (
        post_id INTEGER PRIMARY KEY, evaluation_datetime TEXT, unique_score REAL,
        most_similar_post_id INTEGER, max_similarity_score REAL, is_repost INTEGER, penalty REAL
    );
    """)
    cursor.execute("PRAGMA table_info(evaluation_scores);")
    existing_columns = [row['name'] for row in cursor.fetchall()]
    if 'score_breakdown' not in existing_columns:
        cursor.execute("ALTER TABLE evaluation_scores ADD COLUMN score_breakdown TEXT;")
    if 'author_post_count' not in existing_columns:
        cursor.execute("ALTER TABLE evaluation_scores ADD COLUMN author_post_count INTEGER;")
    cursor.execute("""
    CREATE INDEX IF NOT EXISTS idx_post_filter
    ON posts (author_real_gender, author_real_age, author_char_race, author_char_gender);
    """)
    conn.commit()

if __name__ == "__main__":
    main()
