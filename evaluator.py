# -*- coding: utf-8 -*-
import sqlite3
import datetime
import json
import pickle
import logging
import re
import unicodedata
import argparse # argparseã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from typing import List, Dict, Any, Optional, Set

import MeCab
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

import config # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾© ---

class Post:
    """
    postsãƒ†ãƒ¼ãƒ–ãƒ«ã®1ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    def __init__(self, db_row: Dict[str, Any]):
        self.post_id: int = db_row['post_id']
        # post_datetimeãŒNoneã®å ´åˆã‚„ç©ºæ–‡å­—åˆ—ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 
        dt_str = db_row.get('post_datetime')
        self.post_datetime: Optional[datetime.datetime] = datetime.datetime.strptime(dt_str, '%Y/%m/%d %H:%M') if dt_str else None
        self.title: str = db_row['title']
        self.purpose: str = db_row['purpose']
        self.original_text: str = db_row['original_text']
        self.author_name: str = db_row['author_name']

        # å¹´ä»£ã¯æ•°å€¤ã¨ã—ã¦æ‰±ã†
        age_str = (db_row['author_real_age'] or '0').replace('ä»£', '').replace('ï¼Ÿ', '0')
        self.author_real_age: int = int(age_str) if age_str.isdigit() else 0

        self.author_real_gender: str = db_row['author_real_gender']
        self.author_char_race: str = db_row['author_char_race']
        self.author_char_gender: str = db_row['author_char_gender']
        self.author_char_job: str = db_row['author_char_job']
        self.server: str = db_row['server']
        self.voice_chat: str = db_row['voice_chat']
        self.server_transfer: str = db_row['server_transfer']
        self.sub_char_ok: bool = bool(db_row['sub_char_ok'])

        # JSONå½¢å¼ã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆ/ã‚»ãƒƒãƒˆã«å¤‰æ›
        self.external_tools: List[str] = json.loads(db_row['external_tools'] or '[]')
        self.playstyle_tags: List[str] = json.loads(db_row['playstyle_tags'] or '[]')
        self.activity_times: List[str] = json.loads(db_row['activity_times'] or '[]')
        self.wish_races: List[str] = json.loads(db_row['wish_races'] or '[]')
        self.wish_char_genders: List[str] = json.loads(db_row['wish_char_genders'] or '[]')
        self.wish_jobs: List[str] = json.loads(db_row['wish_jobs'] or '[]')
        self.wish_real_genders: List[str] = json.loads(db_row['wish_real_genders'] or '[]')
        self.wish_real_ages: List[str] = json.loads(db_row['wish_real_ages'] or '[]')


        # å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆ
        self.all_tags: Set[str] = self._create_all_tags_set()

    def _create_all_tags_set(self) -> Set[str]:
        """è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ã®å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆã™ã‚‹"""
        tags = set()
        tags.update(self.playstyle_tags)
        tags.update(self.activity_times)
        tags.update(self.wish_races)
        tags.update(self.wish_char_genders)
        tags.update(self.wish_jobs)
        tags.update(self.wish_real_genders)
        tags.update(self.wish_real_ages)
        tags.update(self.external_tools)
        if self.voice_chat:
            tags.add(f"VC:{self.voice_chat}")
        if self.server_transfer:
            tags.add(self.server_transfer)
        if self.sub_char_ok:
            tags.add("ã‚µãƒ–ã‚­ãƒ£ãƒ©å¯")
        return tags

    def __repr__(self) -> str:
        return f"<Post id={self.post_id} name='{self.author_name}'>"

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ (è©•ä¾¡å¯¾è±¡å–å¾—) ---

def get_posts_to_evaluate(conn: sqlite3.Connection, args: argparse.Namespace) -> List[Post]:
    """
    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«åŸºã¥ã„ã¦è©•ä¾¡å¯¾è±¡ã®æŠ•ç¨¿ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
    """
    cursor = conn.cursor()

    if args.post_id:
        logging.info(f"--post-idãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚æŠ•ç¨¿ID: {args.post_id} ã‚’è©•ä¾¡ã—ã¾ã™ã€‚")
        cursor.execute("SELECT * FROM posts WHERE post_id = ?", (args.post_id,))
        rows = cursor.fetchall()
    elif args.re_evaluate_all:
        logging.info("--re-evaluate-allãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚å…¨æŠ•ç¨¿ã‚’å†è©•ä¾¡ã—ã¾ã™ã€‚")
        # è©•ä¾¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒªã‚¢
        cursor.execute("DELETE FROM evaluation_scores")
        conn.commit()
        logging.info("æ—¢å­˜ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        cursor.execute("SELECT * FROM posts ORDER BY post_datetime ASC")
        rows = cursor.fetchall()
    else:
        logging.info("æœªè©•ä¾¡ã®æŠ•ç¨¿ã‚’ã™ã¹ã¦è©•ä¾¡ã—ã¾ã™ã€‚ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰")
        cursor.execute("""
            SELECT p.* FROM posts p
            LEFT JOIN evaluation_scores es ON p.post_id = es.post_id
            WHERE es.post_id IS NULL
            ORDER BY p.post_datetime ASC
        """)
        rows = cursor.fetchall()

    if not rows:
        logging.info("è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    target_posts = [Post(dict(row)) for row in rows]
    logging.info(f"{len(target_posts)} ä»¶ã®æŠ•ç¨¿ã‚’è©•ä¾¡å¯¾è±¡ã¨ã—ã¦å–å¾—ã—ã¾ã—ãŸã€‚")
    return target_posts

def get_all_original_texts(conn: sqlite3.Connection) -> List[str]:
    """
    postsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨ã¦ã®original_textã‚’å–å¾—ã™ã‚‹
    """
    cursor = conn.cursor()
    cursor.execute("SELECT original_text FROM posts ORDER BY post_datetime ASC")
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
    return [row['original_text'] for row in cursor.fetchall() if row['original_text']]

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢é€£ ---

def normalize_text(text: str) -> str:
    """
    æ„å‘³å†…å®¹åˆ†æã®ãŸã‚ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹
    """
    if not text: return ""
    text = unicodedata.normalize('NFKC', text) # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«ã€ã‚«ã‚¿ã‚«ãƒŠã‚’å…¨è§’ã«
    text = text.lower() # å°æ–‡å­—ã«å¤‰æ›
    text = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ]', '', text) # ä¸€èˆ¬çš„ãªå¥èª­ç‚¹ã‚’é™¤å»
    text = re.sub(r'[^\w\s]', ' ', text) # ãã®ä»–ã®è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
    text = re.sub(r'\s+', ' ', text).strip() # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    return text

def tokenize_with_mecab(text: str) -> str:
    """
    MeCabã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ã‹ã¡æ›¸ãã™ã‚‹ (TfidfVectorizerã«ç›´æ¥æ¸¡ã›ã‚‹ã‚ˆã†ã«ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’è¿”ã™)
    """
    # mecabrcã®ãƒ‘ã‚¹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
    tagger = MeCab.Tagger("-Owakati -r /etc/mecabrc")
    return tagger.parse(text).strip()


def prepare_vectorizer(conn: sqlite3.Connection) -> Optional[TfidfVectorizer]:
    """
    TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’æº–å‚™ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    try:
        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’èª­ã¿è¾¼ã¿
        with open(config.VECTORIZER_PATH, 'rb') as f:
            vectorizer = pickle.load(f)
        logging.info(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{config.VECTORIZER_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return vectorizer
    except FileNotFoundError:
        logging.info("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å–å¾—
        corpus = get_all_original_texts(conn)
        if not corpus:
            logging.error("ã‚³ãƒ¼ãƒ‘ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚postsãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        logging.info(f"{len(corpus)}ä»¶ã®æŠ•ç¨¿ã‚’ã‚³ãƒ¼ãƒ‘ã‚¹ã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚")

        # æ­£è¦åŒ–å‡¦ç†
        normalized_corpus = [normalize_text(text) for text in corpus]

        # TfidfVectorizerã®åˆæœŸåŒ–ã¨å­¦ç¿’
        # tokenizerã«MeCabã®é–¢æ•°ã‚’æŒ‡å®šã—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚‚å®šç¾©
        stop_words = ['ã§ã™', 'ã¾ã™', 'ã®', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¨', 'ã‚‚', 'ã§', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹']
        vectorizer = TfidfVectorizer(tokenizer=tokenize_with_mecab, stop_words=stop_words)

        vectorizer.fit(normalized_corpus)

        # å­¦ç¿’æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ä¿å­˜
        with open(config.VECTORIZER_PATH, 'wb') as f:
            pickle.dump(vectorizer, f)
        logging.info(f"å­¦ç¿’æ¸ˆã¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{config.VECTORIZER_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

        return vectorizer
    except Exception as e:
        logging.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 1: é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---

def get_candidate_posts(conn: sqlite3.Connection, target_post: Post) -> List[Post]:
    """
    é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŠ•ç¨¿ã¨é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒä¸€è‡´ã™ã‚‹å€™è£œæŠ•ç¨¿ã‚’DBã‹ã‚‰å–å¾—
    """
    cursor = conn.cursor()
    try:
        query = """
            SELECT * FROM posts
            WHERE author_real_gender = ?
              AND author_real_age = ?
              AND author_char_race = ?
              AND author_char_gender = ?
              AND post_id != ?
              AND post_datetime < ?
        """
        params = (
            target_post.author_real_gender,
            f"{target_post.author_real_age}ä»£" if target_post.author_real_age != 0 else "ï¼Ÿä»£",
            target_post.author_char_race,
            target_post.author_char_gender,
            target_post.post_id,
            target_post.post_datetime.strftime('%Y/%m/%d %H:%M') if target_post.post_datetime else ''
        )
        cursor.execute(query, params)
        rows = cursor.fetchall()
        # Postã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç”Ÿæˆæ™‚ã«Noneãƒã‚§ãƒƒã‚¯ãŒå¼·åŒ–ã•ã‚ŒãŸãŸã‚ã€ã“ã“ã§ã®è¿½åŠ ãƒã‚§ãƒƒã‚¯ã¯ä¸è¦
        candidate_posts = [Post(dict(row)) for row in rows]
        logging.info(f"é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Š {len(candidate_posts)} ä»¶ã®æ¯”è¼ƒå€™è£œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        return candidate_posts
    except sqlite3.Error as e:
        logging.error(f"å€™è£œæŠ•ç¨¿ã®å–å¾—ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 5: çµæœã®ä¿å­˜ã¨å‡ºåŠ› ---

def save_evaluation_score(conn: sqlite3.Connection, post_id: int, result: Dict[str, Any]):
    """
    è©•ä¾¡çµæœã‚’ evaluation_scores ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹
    """
    cursor = conn.cursor()
    try:
        query = """
            INSERT OR REPLACE INTO evaluation_scores (
                post_id, evaluation_datetime, unique_score, most_similar_post_id,
                max_similarity_score, is_repost, penalty,
                score_breakdown, author_post_count
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
        """
        # score_breakdownã¯JSONæ–‡å­—åˆ—ã«å¤‰æ›
        score_breakdown_json = json.dumps(result.get('score_breakdown', {}), ensure_ascii=False)

        params = (
            post_id,
            datetime.datetime.now().isoformat(),
            result.get('unique_score'),
            result.get('most_similar_post_id'),
            result.get('max_similarity_score'),
            result.get('is_repost'),
            result.get('penalty'),
            score_breakdown_json,
            result.get('author_post_count')
        )
        cursor.execute(query, params)
        conn.commit()
        logging.info(f"Post ID {post_id} ã®è©•ä¾¡çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except sqlite3.Error as e:
        logging.error(f"è©•ä¾¡çµæœã®ä¿å­˜ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def print_summary(target_post: Post, result: Dict[str, Any]):
    """
    è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã™ã‚‹
    """
    print("\n" + "="*60)
    print(" " * 20 + "è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f" è©•ä¾¡å¯¾è±¡æŠ•ç¨¿ID: {target_post.post_id}")
    print(f" æŠ•ç¨¿è€…å:         {target_post.author_name}")
    print(f" æŠ•ç¨¿æ—¥æ™‚:         {target_post.post_datetime.strftime('%Y-%m-%d %H:%M') if target_post.post_datetime else 'N/A'}")
    print("-"*60)
    print(f" ã€æœ€çµ‚ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã€‘: {result['unique_score']:.2f} / 100")
    print("-"*60)
    print(f"   - æœ€å¤§é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢:     {result['max_similarity_score']:.2f}")
    if result['most_similar_post_id']:
        print(f"   - æœ€ã‚‚é¡ä¼¼ã—ãŸæŠ•ç¨¿ID: {result['most_similar_post_id']}")
    else:
        print("   - é¡ä¼¼ã—ãŸæŠ•ç¨¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    print(f"   - å†æŠ•ç¨¿åˆ¤å®š:           {'ã¯ã„' if result['is_repost'] else 'ã„ã„ãˆ'}")
    print(f"   - é©ç”¨ãƒšãƒŠãƒ«ãƒ†ã‚£:       {result['penalty']:.2f}")
    print("="*60 + "\n")


# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 2: é¡ä¼¼åº¦åˆ†æ ---

def calculate_static_profile_score(post_a: Post, post_b: Post) -> float:
    """
    A) é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (45ç‚¹æº€ç‚¹)
    """
    score = 0
    # ãƒªã‚¢ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    age_diff = abs(post_a.author_real_age - post_b.author_real_age)
    score += config.REAL_AGE_SCORE_MAX * (1 - min(5, age_diff) / 5)

    # ã‚­ãƒ£ãƒ©æƒ…å ±
    if post_a.author_char_race == post_b.author_char_race and \
       post_a.author_char_gender == post_b.author_char_gender:
        score += config.CHAR_INFO_SCORE_MAX

    # å¤‰å‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    if post_a.author_char_job == post_b.author_char_job and \
       post_a.author_name == post_b.author_name:
        score += config.VARIABLE_PROFILE_SCORE_MAX

    return score

def calculate_behavioral_pattern_score(post_a: Post, post_b: Post, tag_weights: Dict[str, float]) -> float:
    """
    B) è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (25ç‚¹æº€ç‚¹) - é‡ã¿ä»˜ãJaccardä¿‚æ•°
    """
    intersection_tags = post_a.all_tags.intersection(post_b.all_tags)
    union_tags = post_a.all_tags.union(post_b.all_tags)

    if not union_tags:
        return 0.0

    intersection_weight = sum(tag_weights.get(tag, 0) for tag in intersection_tags)
    union_weight = sum(tag_weights.get(tag, 0) for tag in union_tags)

    if union_weight == 0:
        return 0.0

    weighted_jaccard_index = intersection_weight / union_weight
    return config.BEHAVIORAL_PATTERN_SCORE_MAX * weighted_jaccard_index

def _calculate_tag_weights_from_db(conn: sqlite3.Connection) -> Dict[str, float]:
    """
    DBã‹ã‚‰å…¨ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’å…ƒã«ã€ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼‰
    """
    cursor = conn.cursor()
    tag_counts = {}

    # postsãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    cursor.execute("SELECT * FROM posts")
    rows = cursor.fetchall()
    all_posts = [Post(dict(row)) for row in rows]

    # å„æŠ•ç¨¿ã®å…¨ã‚¿ã‚°é›†åˆã‚’èµ°æŸ»ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    for post in all_posts:
        for tag in post.all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1

    # é‡ã¿ã‚’è¨ˆç®—: w(t) = 1 / log(1 + (å‡ºç¾å›æ•°))
    tag_weights = {tag: 1 / np.log(1 + count) for tag, count in tag_counts.items()}
    logging.info(f"{len(tag_weights)}å€‹ã®ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚")
    return tag_weights

def prepare_tag_weights(conn: sqlite3.Connection) -> Dict[str, float]:
    """
    ã‚¿ã‚°ã®é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    try:
        with open(config.TAG_WEIGHTS_PATH, 'rb') as f:
            tag_weights = pickle.load(f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{config.TAG_WEIGHTS_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return tag_weights
    except FileNotFoundError:
        logging.info("ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")
        tag_weights = _calculate_tag_weights_from_db(conn)

        with open(config.TAG_WEIGHTS_PATH, 'wb') as f:
            pickle.dump(tag_weights, f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{config.TAG_WEIGHTS_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return tag_weights
    except Exception as e:
        logging.error(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {}

def create_stylistic_feature_vector(text: str) -> np.ndarray:
    """
    æ–‡ä½“ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    if not text:
        return np.zeros(8) # 8æ¬¡å…ƒã®ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«

    # 1. å…¨è§’/åŠè§’ä½¿ç”¨æ¯”ç‡
    zen_count = len(re.findall(r'[ï¼-ï½]', text))
    han_count = len(re.findall(r'[!-~]', text))
    zen_han_ratio = zen_count / (zen_count + han_count) if (zen_count + han_count) > 0 else 0

    # 2. ç‰¹æ®Šè¨˜å·ãƒ»çµµæ–‡å­—ä½¿ç”¨é »åº¦
    special_chars = len(re.findall(r'[ï¼ï¼Ÿwâ™ªâœ¨ğŸ€ğŸ€ğŸ°ğŸ§¸ğŸ’Œ]', text))
    special_char_freq = special_chars / len(text)

    # 3. å¥èª­ç‚¹å¾Œã‚¹ãƒšãƒ¼ã‚¹ç‡
    punctuations = re.findall(r'[ã€‚ã€]', text)
    punc_with_space = len(re.findall(r'([ã€‚ã€])\s', text))
    punc_space_ratio = punc_with_space / len(punctuations) if punctuations else 0

    # 4. æ”¹è¡Œé »åº¦
    newline_freq = text.count('\n') / len(text)

    # 5-7. å„æ–‡å­—ç¨®ã®æ¯”ç‡
    hiragana_ratio = len(re.findall(r'[\u3040-\u309F]', text)) / len(text)
    katakana_ratio = len(re.findall(r'[\u30A0-\u30FF]', text)) / len(text)
    kanji_ratio = len(re.findall(r'[\u4E00-\u9FFF]', text)) / len(text)

    # 8. é›£èª­æ¼¢å­—ä½¿ç”¨ç‡ (ç°¡æ˜“ç‰ˆ: ã“ã“ã§ã¯å›ºå®šå€¤0ã¨ã™ã‚‹)
    # æœ¬æ¥ã¯å¸¸ç”¨æ¼¢å­—ãƒªã‚¹ãƒˆã¨ã®æ¯”è¼ƒãŒå¿…è¦ã ãŒã€ä»Šå›ã¯çœç•¥
    difficult_kanji_ratio = 0.0

    return np.array([
        zen_han_ratio, special_char_freq, punc_space_ratio, newline_freq,
        hiragana_ratio, katakana_ratio, kanji_ratio, difficult_kanji_ratio
    ])

def calculate_linguistic_fingerprint_score(
    post_a: Post, post_b: Post, vectorizer: TfidfVectorizer
) -> Dict[str, float]:
    """
    C) è¨€èªçš„æŒ‡ç´‹é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¨ã€ãã®å†…è¨³ã‚’è¨ˆç®—ã™ã‚‹
    """
    # 1. æ„å‘³å†…å®¹ã®é¡ä¼¼åº¦
    texts = [normalize_text(post_a.original_text), normalize_text(post_b.original_text)]
    tfidf_vectors = vectorizer.transform(texts)
    semantic_similarity = cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])[0][0]
    semantic_score = config.SEMANTIC_SCORE_MAX * semantic_similarity

    # 2. æ–‡ä½“æŒ‡ç´‹ã®é¡ä¼¼åº¦
    style_vec_a = create_stylistic_feature_vector(post_a.original_text)
    style_vec_b = create_stylistic_feature_vector(post_b.original_text)

    euclidean_dist = np.linalg.norm(style_vec_a - style_vec_b)
    stylistic_similarity = np.exp(-0.5 * euclidean_dist)
    stylistic_score = config.STYLISTIC_SCORE_MAX * stylistic_similarity

    # 3. ä¿¡é ¼åº¦ä¿‚æ•°ã®é©ç”¨
    reliability_factor = min(1.0, len(post_a.original_text) / config.RELIABILITY_TEXT_LENGTH if post_a.original_text else 0.0)

    final_linguistic_score = (semantic_score + stylistic_score) * reliability_factor

    return {
        "total": final_linguistic_score,
        "semantic": semantic_score,
        "stylistic": stylistic_score
    }

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 3 & 4: è£œæ­£ã€åˆ¤å®šã€æœ€çµ‚ã‚¹ã‚³ã‚¢ç®—å‡º ---

def trace_repost_chain(conn: sqlite3.Connection, start_post_id: int, visited: Set[int]) -> int:
    """
    å†æŠ•ç¨¿ã®é€£é–ã‚’å†å¸°çš„ã«é¡ã‚Šã€ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚µã‚¤ã‚ºï¼ˆï¼æŠ•ç¨¿å›æ•°ï¼‰ã‚’è¿”ã™ã€‚
    """
    if start_post_id is None or start_post_id in visited:
        return 0

    visited.add(start_post_id)
    cursor = conn.cursor()
    cursor.execute("""
        SELECT most_similar_post_id, is_repost
        FROM evaluation_scores
        WHERE post_id = ?
    """, (start_post_id,))
    result = cursor.fetchone()

    # è©•ä¾¡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒãªã„ã€ã¾ãŸã¯å†æŠ•ç¨¿ã§ãªã„å ´åˆã¯ã€ã“ã®æŠ•ç¨¿ãŒãƒã‚§ãƒ¼ãƒ³ã®èµ·ç‚¹
    if not result or not result['is_repost']:
        return 1

    # å†å¸°çš„ã«æ¬¡ã®é¡ä¼¼æŠ•ç¨¿ã‚’æ¢ç´¢
    return 1 + trace_repost_chain(conn, result['most_similar_post_id'], visited)

def evaluate_post(target_post: Post, candidates: List[Post], vectorizer: TfidfVectorizer, tag_weights: Dict[str, float], conn: sqlite3.Connection) -> Dict[str, Any]:
    """
    1ä»¶ã®æŠ•ç¨¿ã‚’è©•ä¾¡ã—ã€è©•ä¾¡çµæœã‚’è¾æ›¸ã§è¿”ã™
    """
    score_breakdown = {}

    if not candidates:
        logging.info("æ¯”è¼ƒå¯¾è±¡ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã¯100ç‚¹ã¨ã—ã¾ã™ã€‚")
        return {
            "unique_score": 100.0, "most_similar_post_id": None, "max_similarity_score": 0.0,
            "is_repost": 0, "penalty": 0.0, "author_post_count": 1,
            "score_breakdown": {"reason": "No candidates"}
        }

    max_similarity_score = 0.0
    most_similar_post_id = None
    best_candidate_scores = {}

    for candidate_post in candidates:
        static_score = calculate_static_profile_score(target_post, candidate_post)
        behavioral_score = calculate_behavioral_pattern_score(target_post, candidate_post, tag_weights)
        linguistic_scores = calculate_linguistic_fingerprint_score(target_post, candidate_post, vectorizer)
        base_similarity_score = static_score + behavioral_score + linguistic_scores["total"]

        consistency_bonus, inconsistency_penalty = 0, 0
        is_server_fake = (re.search(r'(ã‚µãƒ¼ãƒãƒ¼|é¯–).*?(ãƒ•ã‚§ã‚¤ã‚¯|å½è£…|ãƒ€ãƒŸãƒ¼)', target_post.original_text or "", re.I) or
                          re.search(r'(ã‚µãƒ¼ãƒãƒ¼|é¯–).*?(ãƒ•ã‚§ã‚¤ã‚¯|å½è£…|ãƒ€ãƒŸãƒ¼)', candidate_post.original_text or "", re.I))

        if not is_server_fake and target_post.server == candidate_post.server:
            consistency_bonus = config.CONSISTENCY_BONUS
        current_similarity_score = base_similarity_score + consistency_bonus

        if current_similarity_score >= config.REPOST_THRESHOLD:
            if linguistic_scores["semantic"] >= 13.0 and linguistic_scores["stylistic"] <= 7.0:
                inconsistency_penalty = config.INCONSISTENCY_PENALTY
                current_similarity_score += inconsistency_penalty

        if current_similarity_score > max_similarity_score:
            max_similarity_score = current_similarity_score
            most_similar_post_id = candidate_post.post_id
            best_candidate_scores = {
                "static_score": static_score, "behavioral_score": behavioral_score,
                "linguistic_score_total": linguistic_scores["total"], "linguistic_semantic": linguistic_scores["semantic"],
                "linguistic_stylistic": linguistic_scores["stylistic"], "consistency_bonus": consistency_bonus,
                "inconsistency_penalty": inconsistency_penalty, "base_similarity": base_similarity_score,
            }

    score_breakdown.update(best_candidate_scores)

    static_score_of_most_similar = best_candidate_scores.get("static_score", 0)
    is_repost = 1 if max_similarity_score >= config.REPOST_THRESHOLD and \
                     static_score_of_most_similar >= config.REPOST_STATIC_SCORE_THRESHOLD else 0

    author_post_count = 1
    if is_repost and most_similar_post_id is not None:
        # å†å¸°çš„ã«å†æŠ•ç¨¿ã®é€£é–ã‚’è¾¿ã‚Šã€æŠ•ç¨¿å›æ•°ã‚’æ±ºå®šã™ã‚‹
        # ã“ã“ã§ most_similar_post_id ã‚’èµ·ç‚¹ã¨ã™ã‚‹ã“ã¨ã§ã€éå»ã®æŠ•ç¨¿ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚µã‚¤ã‚ºã‚’å–å¾—ã§ãã‚‹
        # ä»Šå›ã®æŠ•ç¨¿è‡ªèº«ã‚‚ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸€éƒ¨ãªã®ã§ã€+1ã™ã‚‹
        past_cluster_size = trace_repost_chain(conn, most_similar_post_id, visited=set())
        author_post_count = past_cluster_size + 1
        logging.info(f"é¡ä¼¼æŠ•ç¨¿(ID: {most_similar_post_id})ã®å†æŠ•ç¨¿ãƒã‚§ãƒ¼ãƒ³ã‚’é¡ã‚Šã€éå»ã®æŠ•ç¨¿å›æ•°ã‚’ {past_cluster_size} ã¨æ–­å®šã€‚ä»Šå›ã¯ {author_post_count} å›ç›®ã®æŠ•ç¨¿ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚")
    else:
        # å†æŠ•ç¨¿ã§ã¯ãªã„å ´åˆã€ã“ã‚ŒãŒæ–°ã—ã„æŠ•ç¨¿ã‚¯ãƒ©ã‚¹ã‚¿ã®èµ·ç‚¹ã¨ãªã‚‹
        logging.info("å†æŠ•ç¨¿ã§ã¯ãªã„ãŸã‚ã€æŠ•ç¨¿å›æ•°ã¯ 1 ã¨ã—ã¾ã™ã€‚")

    penalty = 0.0
    if is_repost and target_post.post_datetime:
        N = author_post_count
        D = (datetime.datetime.now() - target_post.post_datetime).days
        base_penalty = config.BASE_PENALTY_COEFFICIENT - (config.PENALTY_PER_REPOST * (N - 1))
        recovery_limit = max(config.MIN_RECOVERY_LIMIT, config.MAX_RECOVERY_LIMIT - (N - 2))
        time_recovery_bonus = min(recovery_limit, (D / config.DAYS_FOR_FULL_RECOVERY) * recovery_limit)
        penalty = base_penalty + time_recovery_bonus

    score_breakdown.update({
        "final_penalty": penalty,
        "final_author_post_count": author_post_count
    })

    tentative_score = 100 - max_similarity_score + penalty
    unique_score = max(0, min(100, tentative_score))

    return {
        "unique_score": unique_score, "most_similar_post_id": most_similar_post_id,
        "max_similarity_score": max_similarity_score, "is_repost": is_repost,
        "penalty": penalty, "author_post_count": author_post_count,
        "score_breakdown": score_breakdown
    }

def setup_database_tables(conn: sqlite3.Connection):
    """
    å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆãƒ»æ›´æ–°ã™ã‚‹ã€‚
    """
    cursor = conn.cursor()

    # è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ« (åŸºæœ¬æ§‹é€ )
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS evaluation_scores (
        post_id INTEGER PRIMARY KEY,
        evaluation_datetime TEXT,
        unique_score REAL,
        most_similar_post_id INTEGER,
        max_similarity_score REAL,
        is_repost INTEGER,
        penalty REAL
    );
    """)

    # --- ã‚«ãƒ©ãƒ ã®è¿½åŠ  (å†ªç­‰æ€§ã‚’æ‹…ä¿) ---
    # ç¾åœ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
    cursor.execute("PRAGMA table_info(evaluation_scores);")
    existing_columns = [row['name'] for row in cursor.fetchall()]

    # score_breakdown ã‚«ãƒ©ãƒ ã®è¿½åŠ 
    if 'score_breakdown' not in existing_columns:
        cursor.execute("ALTER TABLE evaluation_scores ADD COLUMN score_breakdown TEXT;")
        logging.info("'score_breakdown' ã‚«ãƒ©ãƒ ã‚’ evaluation_scores ãƒ†ãƒ¼ãƒ–ãƒ«ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")

    # author_post_count ã‚«ãƒ©ãƒ ã®è¿½åŠ 
    if 'author_post_count' not in existing_columns:
        cursor.execute("ALTER TABLE evaluation_scores ADD COLUMN author_post_count INTEGER;")
        logging.info("'author_post_count' ã‚«ãƒ©ãƒ ã‚’ evaluation_scores ãƒ†ãƒ¼ãƒ–ãƒ«ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")

    # Step 1 é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    cursor.execute("""
    CREATE INDEX IF NOT EXISTS idx_post_filter
    ON posts (author_real_gender, author_real_age, author_char_race, author_char_gender);
    """)

    conn.commit()
    logging.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    """
    è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    parser = argparse.ArgumentParser(description="æŠ•ç¨¿ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    parser.add_argument('--post-id', type=int, help='æŒ‡å®šã—ãŸpost_idã®æŠ•ç¨¿ã®ã¿ã‚’è©•ä¾¡ã™ã‚‹')
    parser.add_argument('--re-evaluate-all', action='store_true', help='ã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å¼·åˆ¶çš„ã«å†è©•ä¾¡ã™ã‚‹')
    args = parser.parse_args()

    logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é–‹å§‹ã—ã¾ã™ ---")

    try:
        with sqlite3.connect(config.DB_NAME) as conn:
            conn.row_factory = sqlite3.Row
            setup_database_tables(conn)

            # Step 1: è©•ä¾¡å¯¾è±¡ã®é¸å®š (argparseå¯¾å¿œ)
            target_posts = get_posts_to_evaluate(conn, args)
            if not target_posts:
                logging.info("è©•ä¾¡å¯¾è±¡ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                return

            # Step 2: TF-IDFã‚³ãƒ¼ãƒ‘ã‚¹ã¨ã‚¿ã‚°é‡ã¿ã®æº–å‚™
            vectorizer = prepare_vectorizer(conn)
            if not vectorizer:
                logging.error("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return
            tag_weights = prepare_tag_weights(conn)


            # Step 3: è©•ä¾¡ãƒ«ãƒ¼ãƒ—
            for i, target_post in enumerate(target_posts):
                logging.info(f"--- ({i+1}/{len(target_posts)}) post_id: {target_post.post_id} ã®è©•ä¾¡ã‚’é–‹å§‹ ---")

                # Step 3-1 (é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
                candidate_posts = get_candidate_posts(conn, target_post)

                # Step 3-2 (é¡ä¼¼åº¦åˆ†æã€œæœ€çµ‚ã‚¹ã‚³ã‚¢ç®—å‡º)
                evaluation_result = evaluate_post(target_post, candidate_posts, vectorizer, tag_weights, conn)

                # Step 3-3 (çµæœã®ä¿å­˜ã¨å‡ºåŠ›)
                save_evaluation_score(conn, target_post.post_id, evaluation_result)
                print_summary(target_post, evaluation_result)

    except sqlite3.Error as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
    except Exception as e:
        logging.error(f"ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
    finally:
        logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ ---")

if __name__ == "__main__":
    main()
