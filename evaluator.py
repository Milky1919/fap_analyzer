# -*- coding: utf-8 -*-
import sqlite3
import datetime
import json
import pickle
import logging
import re
import unicodedata
import subprocess
from typing import List, Dict, Any, Optional, Set

import MeCab
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

# --- å®šæ•°å®šç¾© ---
DB_NAME = "fap_posts.db"
VECTORIZER_PATH = "vectorizer.pkl"
TAG_WEIGHTS_PATH = "tag_weights.pkl"

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾© ---

class Post:
    """
    postsãƒ†ãƒ¼ãƒ–ãƒ«ã®1ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    def __init__(self, db_row: Dict[str, Any]):
        self.post_id: int = db_row['post_id']
        self.post_datetime: datetime.datetime = datetime.datetime.strptime(db_row['post_datetime'], '%Y/%m/%d %H:%M')
        self.title: str = db_row['title']
        self.purpose: str = db_row['purpose']
        self.original_text: str = db_row['original_text']
        self.author_name: str = db_row['author_name']

        # å¹´ä»£ã¯æ•°å€¤ã¨ã—ã¦æ‰±ã†
        age_str = db_row['author_real_age'].replace('ä»£', '').replace('ï¼Ÿ', '0')
        self.author_real_age: int = int(age_str) if age_str.isdigit() else 0

        self.author_real_gender: str = db_row['author_real_gender']
        self.author_char_race: str = db_row['author_char_race']
        self.author_char_gender: str = db_row['author_char_gender']
        self.author_char_job: str = db_row['author_char_job']
        self.server: str = db_row['server']
        self.voice_chat: str = db_row['voice_chat']
        self.server_transfer: str = db_row['server_transfer']
        self.sub_char_ok: bool = bool(db_row['sub_char_ok'])

        # JSONå½¢å¼ã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆ/ã‚»ãƒƒãƒˆã«å¤‰æ›
        self.external_tools: List[str] = json.loads(db_row['external_tools'])
        self.playstyle_tags: List[str] = json.loads(db_row['playstyle_tags'])
        self.activity_times: List[str] = json.loads(db_row['activity_times'])
        self.wish_races: List[str] = json.loads(db_row['wish_races'])
        self.wish_char_genders: List[str] = json.loads(db_row['wish_char_genders'])
        self.wish_jobs: List[str] = json.loads(db_row['wish_jobs'])
        self.wish_real_genders: List[str] = json.loads(db_row['wish_real_genders'])
        self.wish_real_ages: List[str] = json.loads(db_row['wish_real_ages'])

        # å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆ
        self.all_tags: Set[str] = self._create_all_tags_set()

    def _create_all_tags_set(self) -> Set[str]:
        """è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ã®å…¨ã‚¿ã‚°é›†åˆã‚’ç”Ÿæˆã™ã‚‹"""
        tags = set()
        tags.update(self.playstyle_tags)
        tags.update(self.activity_times)
        tags.update(self.wish_races)
        tags.update(self.wish_char_genders)
        tags.update(self.wish_jobs)
        tags.update(self.wish_real_genders)
        tags.update(self.wish_real_ages)
        tags.update(self.external_tools)
        if self.voice_chat:
            tags.add(f"VC:{self.voice_chat}")
        if self.server_transfer:
            tags.add(self.server_transfer)
        if self.sub_char_ok:
            tags.add("ã‚µãƒ–ã‚­ãƒ£ãƒ©å¯")
        return tags

    def __repr__(self) -> str:
        return f"<Post id={self.post_id} name='{self.author_name}'>"

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ ---

def get_all_original_texts(conn: sqlite3.Connection) -> List[str]:
    """
    postsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨ã¦ã®original_textã‚’å–å¾—ã™ã‚‹
    """
    cursor = conn.cursor()
    cursor.execute("SELECT original_text FROM posts ORDER BY post_datetime ASC")
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
    return [row['original_text'] for row in cursor.fetchall() if row['original_text']]

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢é€£ ---

def normalize_text(text: str) -> str:
    """
    æ„å‘³å†…å®¹åˆ†æã®ãŸã‚ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹
    """
    text = unicodedata.normalize('NFKC', text) # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«ã€ã‚«ã‚¿ã‚«ãƒŠã‚’å…¨è§’ã«
    text = text.lower() # å°æ–‡å­—ã«å¤‰æ›
    text = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ]', '', text) # ä¸€èˆ¬çš„ãªå¥èª­ç‚¹ã‚’é™¤å»
    text = re.sub(r'[^\w\s]', ' ', text) # ãã®ä»–ã®è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
    text = re.sub(r'\s+', ' ', text).strip() # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    return text

def tokenize_with_mecab(text: str) -> str:
    """
    MeCabã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ã‹ã¡æ›¸ãã™ã‚‹ (TfidfVectorizerã«ç›´æ¥æ¸¡ã›ã‚‹ã‚ˆã†ã«ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’è¿”ã™)
    """
    # Dockerfileã§ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆã—ã¦ã„ã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªãƒ‘ã‚¹æŒ‡å®šã¯ä¸è¦
    tagger = MeCab.Tagger("-Owakati")
    return tagger.parse(text).strip()


def prepare_vectorizer(conn: sqlite3.Connection) -> Optional[TfidfVectorizer]:
    """
    TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’æº–å‚™ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    try:
        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’èª­ã¿è¾¼ã¿
        with open(VECTORIZER_PATH, 'rb') as f:
            vectorizer = pickle.load(f)
        logging.info(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{VECTORIZER_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return vectorizer
    except FileNotFoundError:
        logging.info("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å–å¾—
        corpus = get_all_original_texts(conn)
        if not corpus:
            logging.error("ã‚³ãƒ¼ãƒ‘ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚postsãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        logging.info(f"{len(corpus)}ä»¶ã®æŠ•ç¨¿ã‚’ã‚³ãƒ¼ãƒ‘ã‚¹ã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚")

        # æ­£è¦åŒ–å‡¦ç†
        normalized_corpus = [normalize_text(text) for text in corpus]

        # TfidfVectorizerã®åˆæœŸåŒ–ã¨å­¦ç¿’
        # tokenizerã«MeCabã®é–¢æ•°ã‚’æŒ‡å®šã—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚‚å®šç¾©
        stop_words = ['ã§ã™', 'ã¾ã™', 'ã®', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¨', 'ã‚‚', 'ã§', 'ã„ã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹']
        vectorizer = TfidfVectorizer(tokenizer=tokenize_with_mecab, stop_words=stop_words)

        vectorizer.fit(normalized_corpus)

        # å­¦ç¿’æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ä¿å­˜
        with open(VECTORIZER_PATH, 'wb') as f:
            pickle.dump(vectorizer, f)
        logging.info(f"å­¦ç¿’æ¸ˆã¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ '{VECTORIZER_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

        return vectorizer
    except Exception as e:
        logging.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def get_target_post(conn: sqlite3.Connection) -> Optional[Post]:
    """
    è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹æœ€æ–°ã®æœªè©•ä¾¡æŠ•ç¨¿ã‚’1ä»¶å–å¾—ã—ã€Postã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è¿”ã™ã€‚
    """
    cursor = conn.cursor()
    try:
        cursor.execute("""
            SELECT p.*
            FROM posts p
            LEFT JOIN evaluation_scores es ON p.post_id = es.post_id
            WHERE es.post_id IS NULL
            ORDER BY p.post_datetime DESC
            LIMIT 1;
        """)
        row = cursor.fetchone()

        if row:
            logging.info(f"è©•ä¾¡å¯¾è±¡ã®æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: post_id={row['post_id']}")
            return Post(dict(row))
        else:
            logging.info("è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹æ–°ã—ã„æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
    except sqlite3.Error as e:
        logging.error(f"è©•ä¾¡å¯¾è±¡ã®æŠ•ç¨¿å–å¾—ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 1: é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---

def get_candidate_posts(conn: sqlite3.Connection, target_post: Post) -> List[Post]:
    """
    é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŠ•ç¨¿ã¨é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒä¸€è‡´ã™ã‚‹å€™è£œæŠ•ç¨¿ã‚’DBã‹ã‚‰å–å¾—
    """
    cursor = conn.cursor()
    try:
        query = """
            SELECT * FROM posts
            WHERE author_real_gender = ?
              AND author_real_age = ?
              AND author_char_race = ?
              AND author_char_gender = ?
              AND post_id != ?
        """
        params = (
            target_post.author_real_gender,
            f"{target_post.author_real_age}ä»£" if target_post.author_real_age != 0 else "ï¼Ÿä»£",
            target_post.author_char_race,
            target_post.author_char_gender,
            target_post.post_id
        )
        cursor.execute(query, params)
        rows = cursor.fetchall()
        candidate_posts = [Post(dict(row)) for row in rows]
        logging.info(f"é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Š {len(candidate_posts)} ä»¶ã®æ¯”è¼ƒå€™è£œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        return candidate_posts
    except sqlite3.Error as e:
        logging.error(f"å€™è£œæŠ•ç¨¿ã®å–å¾—ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 5: çµæœã®ä¿å­˜ã¨å‡ºåŠ› ---

def save_evaluation_score(conn: sqlite3.Connection, post_id: int, result: Dict[str, Any]):
    """
    è©•ä¾¡çµæœã‚’ evaluation_scores ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹
    """
    cursor = conn.cursor()
    try:
        query = """
            INSERT OR REPLACE INTO evaluation_scores (
                post_id, evaluation_datetime, unique_score, most_similar_post_id,
                max_similarity_score, is_repost, penalty
            ) VALUES (?, ?, ?, ?, ?, ?, ?);
        """
        params = (
            post_id,
            datetime.datetime.now().isoformat(),
            result['unique_score'],
            result['most_similar_post_id'],
            result['max_similarity_score'],
            result['is_repost'],
            result['penalty']
        )
        cursor.execute(query, params)
        conn.commit()
        logging.info(f"Post ID {post_id} ã®è©•ä¾¡çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except sqlite3.Error as e:
        logging.error(f"è©•ä¾¡çµæœã®ä¿å­˜ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def print_summary(target_post: Post, result: Dict[str, Any]):
    """
    è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã™ã‚‹
    """
    print("\n" + "="*60)
    print(" " * 20 + "è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f" è©•ä¾¡å¯¾è±¡æŠ•ç¨¿ID: {target_post.post_id}")
    print(f" æŠ•ç¨¿è€…å:         {target_post.author_name}")
    print(f" æŠ•ç¨¿æ—¥æ™‚:         {target_post.post_datetime.strftime('%Y-%m-%d %H:%M')}")
    print("-"*60)
    print(f" ã€æœ€çµ‚ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã€‘: {result['unique_score']:.2f} / 100")
    print("-"*60)
    print(f"   - æœ€å¤§é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢:     {result['max_similarity_score']:.2f}")
    if result['most_similar_post_id']:
        print(f"   - æœ€ã‚‚é¡ä¼¼ã—ãŸæŠ•ç¨¿ID: {result['most_similar_post_id']}")
    else:
        print("   - é¡ä¼¼ã—ãŸæŠ•ç¨¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    print(f"   - å†æŠ•ç¨¿åˆ¤å®š:           {'ã¯ã„' if result['is_repost'] else 'ã„ã„ãˆ'}")
    print(f"   - é©ç”¨ãƒšãƒŠãƒ«ãƒ†ã‚£:       {result['penalty']:.2f}")
    print("="*60 + "\n")


# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 2: é¡ä¼¼åº¦åˆ†æ ---

def calculate_static_profile_score(post_a: Post, post_b: Post) -> float:
    """
    A) é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (45ç‚¹æº€ç‚¹)
    """
    score = 0
    # ãƒªã‚¢ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« (35ç‚¹)
    age_diff = abs(post_a.author_real_age - post_b.author_real_age)
    score += 35 * (1 - min(5, age_diff) / 5)

    # ã‚­ãƒ£ãƒ©æƒ…å ± (5ç‚¹)
    if post_a.author_char_race == post_b.author_char_race and \
       post_a.author_char_gender == post_b.author_char_gender:
        score += 5

    # å¤‰å‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« (5ç‚¹)
    if post_a.author_char_job == post_b.author_char_job and \
       post_a.author_name == post_b.author_name:
        score += 5

    return score

def calculate_behavioral_pattern_score(post_a: Post, post_b: Post, tag_weights: Dict[str, float]) -> float:
    """
    B) è¡Œå‹•ãƒ»å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (25ç‚¹æº€ç‚¹) - é‡ã¿ä»˜ãJaccardä¿‚æ•°
    """
    intersection_tags = post_a.all_tags.intersection(post_b.all_tags)
    union_tags = post_a.all_tags.union(post_b.all_tags)

    if not union_tags:
        return 0.0

    intersection_weight = sum(tag_weights.get(tag, 0) for tag in intersection_tags)
    union_weight = sum(tag_weights.get(tag, 0) for tag in union_tags)

    if union_weight == 0:
        return 0.0

    weighted_jaccard_index = intersection_weight / union_weight
    return 25 * weighted_jaccard_index

def _calculate_tag_weights_from_db(conn: sqlite3.Connection) -> Dict[str, float]:
    """
    DBã‹ã‚‰å…¨ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’å…ƒã«ã€ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼‰
    """
    cursor = conn.cursor()
    tag_counts = {}

    # postsãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    cursor.execute("SELECT * FROM posts")
    rows = cursor.fetchall()
    all_posts = [Post(dict(row)) for row in rows]

    # å„æŠ•ç¨¿ã®å…¨ã‚¿ã‚°é›†åˆã‚’èµ°æŸ»ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    for post in all_posts:
        for tag in post.all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1

    # é‡ã¿ã‚’è¨ˆç®—: w(t) = 1 / log(1 + (å‡ºç¾å›æ•°))
    tag_weights = {tag: 1 / np.log(1 + count) for tag, count in tag_counts.items()}
    logging.info(f"{len(tag_weights)}å€‹ã®ã‚¿ã‚°ã®é‡ã¿ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚")
    return tag_weights

def prepare_tag_weights(conn: sqlite3.Connection) -> Dict[str, float]:
    """
    ã‚¿ã‚°ã®é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    try:
        with open(TAG_WEIGHTS_PATH, 'rb') as f:
            tag_weights = pickle.load(f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{TAG_WEIGHTS_PATH}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return tag_weights
    except FileNotFoundError:
        logging.info("ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã«ç”Ÿæˆã—ã¾ã™ã€‚")
        tag_weights = _calculate_tag_weights_from_db(conn)

        with open(TAG_WEIGHTS_PATH, 'wb') as f:
            pickle.dump(tag_weights, f)
        logging.info(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{TAG_WEIGHTS_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return tag_weights
    except Exception as e:
        logging.error(f"ã‚¿ã‚°é‡ã¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {}

def create_stylistic_feature_vector(text: str) -> np.ndarray:
    """
    æ–‡ä½“ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    if not text:
        return np.zeros(8) # 8æ¬¡å…ƒã®ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«

    # 1. å…¨è§’/åŠè§’ä½¿ç”¨æ¯”ç‡
    zen_count = len(re.findall(r'[ï¼-ï½]', text))
    han_count = len(re.findall(r'[!-~]', text))
    zen_han_ratio = zen_count / (zen_count + han_count) if (zen_count + han_count) > 0 else 0

    # 2. ç‰¹æ®Šè¨˜å·ãƒ»çµµæ–‡å­—ä½¿ç”¨é »åº¦
    special_chars = len(re.findall(r'[ï¼ï¼Ÿwâ™ªâœ¨ğŸ€ğŸ€ğŸ°ğŸ§¸ğŸ’Œ]', text))
    special_char_freq = special_chars / len(text)

    # 3. å¥èª­ç‚¹å¾Œã‚¹ãƒšãƒ¼ã‚¹ç‡
    punctuations = re.findall(r'[ã€‚ã€]', text)
    punc_with_space = len(re.findall(r'([ã€‚ã€])\s', text))
    punc_space_ratio = punc_with_space / len(punctuations) if punctuations else 0

    # 4. æ”¹è¡Œé »åº¦
    newline_freq = text.count('\n') / len(text)

    # 5-7. å„æ–‡å­—ç¨®ã®æ¯”ç‡
    hiragana_ratio = len(re.findall(r'[\u3040-\u309F]', text)) / len(text)
    katakana_ratio = len(re.findall(r'[\u30A0-\u30FF]', text)) / len(text)
    kanji_ratio = len(re.findall(r'[\u4E00-\u9FFF]', text)) / len(text)

    # 8. é›£èª­æ¼¢å­—ä½¿ç”¨ç‡ (ç°¡æ˜“ç‰ˆ: ã“ã“ã§ã¯å›ºå®šå€¤0ã¨ã™ã‚‹)
    # æœ¬æ¥ã¯å¸¸ç”¨æ¼¢å­—ãƒªã‚¹ãƒˆã¨ã®æ¯”è¼ƒãŒå¿…è¦ã ãŒã€ä»Šå›ã¯çœç•¥
    difficult_kanji_ratio = 0.0

    return np.array([
        zen_han_ratio, special_char_freq, punc_space_ratio, newline_freq,
        hiragana_ratio, katakana_ratio, kanji_ratio, difficult_kanji_ratio
    ])

def calculate_linguistic_fingerprint_score(
    post_a: Post, post_b: Post, vectorizer: TfidfVectorizer
) -> Dict[str, float]:
    """
    C) è¨€èªçš„æŒ‡ç´‹é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¨ã€ãã®å†…è¨³ã‚’è¨ˆç®—ã™ã‚‹
    """
    # 1. æ„å‘³å†…å®¹ã®é¡ä¼¼åº¦ (15ç‚¹)
    texts = [normalize_text(post_a.original_text), normalize_text(post_b.original_text)]
    tfidf_vectors = vectorizer.transform(texts)
    semantic_similarity = cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])[0][0]
    semantic_score = 15 * semantic_similarity

    # 2. æ–‡ä½“æŒ‡ç´‹ã®é¡ä¼¼åº¦ (15ç‚¹)
    style_vec_a = create_stylistic_feature_vector(post_a.original_text)
    style_vec_b = create_stylistic_feature_vector(post_b.original_text)

    euclidean_dist = np.linalg.norm(style_vec_a - style_vec_b)
    stylistic_similarity = np.exp(-0.5 * euclidean_dist)
    stylistic_score = 15 * stylistic_similarity

    # 3. ä¿¡é ¼åº¦ä¿‚æ•°ã®é©ç”¨
    reliability_factor = min(1.0, len(post_a.original_text) / 100)

    final_linguistic_score = (semantic_score + stylistic_score) * reliability_factor

    return {
        "total": final_linguistic_score,
        "semantic": semantic_score,
        "stylistic": stylistic_score
    }

# --- è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ Step 3 & 4: è£œæ­£ã€åˆ¤å®šã€æœ€çµ‚ã‚¹ã‚³ã‚¢ç®—å‡º ---

def evaluate_post(target_post: Post, candidates: List[Post], vectorizer: TfidfVectorizer, tag_weights: Dict[str, float], conn: sqlite3.Connection) -> Dict[str, Any]:
    """
    1ä»¶ã®æŠ•ç¨¿ã‚’è©•ä¾¡ã—ã€è©•ä¾¡çµæœã‚’è¾æ›¸ã§è¿”ã™
    """
    if not candidates:
        logging.info("æ¯”è¼ƒå¯¾è±¡ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã¯100ç‚¹ã¨ã—ã¾ã™ã€‚")
        return {
            "unique_score": 100.0,
            "most_similar_post_id": None,
            "max_similarity_score": 0.0,
            "is_repost": 0,
            "penalty": 0.0
        }

    max_similarity_score = 0
    most_similar_post_id = None

    # å„å€™è£œã¨æ¯”è¼ƒ
    for candidate_post in candidates:
        # Step 2: å„ç¨®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        static_score = calculate_static_profile_score(target_post, candidate_post)
        behavioral_score = calculate_behavioral_pattern_score(target_post, candidate_post, tag_weights)
        linguistic_scores = calculate_linguistic_fingerprint_score(target_post, candidate_post, vectorizer)

        # Step 3: åŸºç¤é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¨è£œæ­£
        base_similarity_score = static_score + behavioral_score + linguistic_scores["total"]

        # ã‚µãƒ¼ãƒãƒ¼å½è£…ãƒ•ãƒ©ã‚° (ç°¡æ˜“ç‰ˆ)
        is_server_fake_target = bool(re.search(r'(ã‚µãƒ¼ãƒãƒ¼|é¯–).*?(ãƒ•ã‚§ã‚¤ã‚¯|å½è£…|ãƒ€ãƒŸãƒ¼)', target_post.original_text, re.I))
        is_server_fake_candidate = bool(re.search(r'(ã‚µãƒ¼ãƒãƒ¼|é¯–).*?(ãƒ•ã‚§ã‚¤ã‚¯|å½è£…|ãƒ€ãƒŸãƒ¼)', candidate_post.original_text, re.I))

        # ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        if not is_server_fake_target and not is_server_fake_candidate and \
           target_post.server == candidate_post.server:
            base_similarity_score += 5

        current_similarity_score = base_similarity_score

        # ä¸ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•‘æ¸ˆæªç½®ï¼‰
        if current_similarity_score >= 88:
            if linguistic_scores["semantic"] >= 13 and linguistic_scores["stylistic"] <= 7:
                current_similarity_score -= 5

        if current_similarity_score > max_similarity_score:
            max_similarity_score = current_similarity_score
            most_similar_post_id = candidate_post.post_id

    # é«˜ç¢ºåº¦å†æŠ•ç¨¿ã®åˆ¤å®š
    # é™çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚³ã‚¢ã‚‚æ¡ä»¶ã«åŠ ãˆã‚‹
    static_score_of_most_similar = 0
    if most_similar_post_id:
        # most_similar_post_idã«å¯¾å¿œã™ã‚‹candidate_postã‚’è¦‹ã¤ã‘ã¦å†è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        for candidate in candidates:
            if candidate.post_id == most_similar_post_id:
                static_score_of_most_similar = calculate_static_profile_score(target_post, candidate)
                break

    is_repost = 1 if max_similarity_score >= 88 and static_score_of_most_similar >= 35 else 0

    # Step 4: æœ€çµ‚ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã®ç®—å‡º
    penalty = 0.0
    if is_repost:
        # ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—: éå»ã®å†æŠ•ç¨¿å›æ•°Nã‚’DBã‹ã‚‰å–å¾—
        cursor = conn.cursor()
        query = """
            SELECT COUNT(*) FROM evaluation_scores es
            JOIN posts p ON es.post_id = p.post_id
            WHERE p.author_name = ?
              AND p.author_real_gender = ?
              AND p.author_real_age = ?
              AND es.is_repost = 1
        """
        params = (
            target_post.author_name,
            target_post.author_real_gender,
            f"{target_post.author_real_age}ä»£" if target_post.author_real_age != 0 else "ï¼Ÿä»£"
        )
        cursor.execute(query, params)
        past_repost_count = cursor.fetchone()[0]

        N = past_repost_count + 1 # ä»Šå›ã®å†æŠ•ç¨¿åˆ†ã‚’+1
        logging.info(f"éå»ã®å†æŠ•ç¨¿å›æ•°ã¯ {past_repost_count} å›ã€‚ä»Šå›ã®å†æŠ•ç¨¿ã¯ {N} å›ç›®ã¨ã—ã¦ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—ã—ã¾ã™ã€‚")

        D = (datetime.datetime.now() - target_post.post_datetime).days
        base_penalty = -30 - (15 * (N - 1))
        recovery_limit = max(5, 15 - (N - 2))
        time_recovery_bonus = min(recovery_limit, (D / 10) * recovery_limit)
        penalty = base_penalty + time_recovery_bonus

    tentative_score = 100 - max_similarity_score + penalty
    unique_score = max(0, min(100, tentative_score))

    return {
        "unique_score": unique_score,
        "most_similar_post_id": most_similar_post_id,
        "max_similarity_score": max_similarity_score,
        "is_repost": is_repost,
        "penalty": penalty
    }

def setup_database_tables(conn: sqlite3.Connection):
    """
    å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ã€‚
    """
    cursor = conn.cursor()

    # è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS evaluation_scores (
        post_id INTEGER PRIMARY KEY,
        evaluation_datetime TEXT,
        unique_score REAL,
        most_similar_post_id INTEGER,
        max_similarity_score REAL,
        is_repost INTEGER,
        penalty REAL
    );
    """)

    # Step 1 é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    cursor.execute("""
    CREATE INDEX IF NOT EXISTS idx_post_filter
    ON posts (author_real_gender, author_real_age, author_char_race, author_char_gender);
    """)

    conn.commit()
    logging.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    """
    è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é–‹å§‹ã—ã¾ã™ ---")

    try:
        with sqlite3.connect(DB_NAME) as conn:
            conn.row_factory = sqlite3.Row
            setup_database_tables(conn)

            # Step 2: TF-IDFã‚³ãƒ¼ãƒ‘ã‚¹ã®æº–å‚™
            vectorizer = prepare_vectorizer(conn)
            if not vectorizer:
                logging.error("ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return

            # Step 3: è©•ä¾¡å¯¾è±¡ã®é¸å®š
            target_post = get_target_post(conn)
            if not target_post:
                logging.info("è©•ä¾¡å¯¾è±¡ã®æŠ•ç¨¿ãŒãªã„ãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                return

            # Step 4: è©•ä¾¡ã®å®Ÿè¡Œ
            # Step 1 (é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
            candidate_posts = get_candidate_posts(conn, target_post)

            # ã‚¿ã‚°ã®é‡ã¿ã‚’æº–å‚™
            tag_weights = prepare_tag_weights(conn)

            # Step 2-4 (é¡ä¼¼åº¦åˆ†æã€œæœ€çµ‚ã‚¹ã‚³ã‚¢ç®—å‡º)
            evaluation_result = evaluate_post(target_post, candidate_posts, vectorizer, tag_weights, conn)

            # Step 5: çµæœã®ä¿å­˜ã¨å‡ºåŠ›
            save_evaluation_score(conn, target_post.post_id, evaluation_result)
            print_summary(target_post, evaluation_result)

    except sqlite3.Error as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    except Exception as e:
        logging.error(f"ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        logging.info("--- æŠ•ç¨¿è©•ä¾¡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ ---")

if __name__ == "__main__":
    main()
