# スクレイピング処理 実装手順書 (Ver 2.0)

## 1. 目的

このドキュメントは、投稿評価アルゴリズムの分析元、および将来的なAPIでのフィルタリングに利用可能な全てのデータを、対象サイトから網羅的に収集し、ローカルのSQLiteデータベースに保存するPythonスクリプト（`scraper.py`）を実装するための技術仕様を定義するものです。

**達成事項:**

*   指定されたURLからHTMLを取得する。
*   HTMLを解析し、本ドキュメントで定義された全データを抽出する。
*   抽出したデータを、構造化された形でSQLiteデータベースに永続化する。
*   サイトに過度な負荷をかけないよう、適切なリクエスト間隔を遵守する。

## 2. データベース設計

収集したデータを格納するSQLiteデータベース（`fap_posts.db`）のテーブルスキーマを以下に定義します。将来の拡張性も考慮し、取得可能な全てのデータをカラムとして定義します。

*   **テーブル名:** `posts`

```
CREATE TABLE IF NOT EXISTS posts (
    -- 基本情報
    post_id INTEGER PRIMARY KEY,
    post_datetime TEXT,
    title TEXT,
    purpose TEXT, -- フレンド, 相方, その他
    original_text TEXT,

    -- 募集主のプロフィール情報
    author_name TEXT,
    author_real_age TEXT, -- 「20代」「？代」など
    author_real_gender TEXT,
    author_char_race TEXT,
    author_char_gender TEXT,
    author_char_job TEXT,

    -- 募集主のプレイ環境・スタイル
    server TEXT,
    voice_chat TEXT, -- ◯, △, ✕
    external_tools TEXT, -- JSON list
    playstyle_tags TEXT, -- JSON list

    -- 投稿ごとの設定
    server_transfer TEXT, -- 移転不可, 同DC移転可, 移転可
    sub_char_ok INTEGER, -- 0 or 1 (boolean)
    activity_times TEXT, -- JSON list

    -- 希望条件
    wish_races TEXT, -- JSON list
    wish_char_genders TEXT, -- JSON list
    wish_jobs TEXT, -- JSON list
    wish_real_genders TEXT, -- JSON list
    wish_real_ages TEXT -- JSON list
);

```

## 3. 実装方針と処理フロー

1.  **データベース接続:** `sqlite3`ライブラリを使用し、`fap_posts.db`に接続する。`posts`テーブルが存在しない場合は上記スキーマで作成する。
2.  **ページ巡回:** `?p=1` から `?p=143` までのURLをループで生成する。
3.  **HTML取得:** 各ページのURLに対して`requests`ライブラリでGETリクエストを送信する。
4.  **リクエスト間隔:** 1回のリクエストごとに `time.sleep(5)` を実行し、5秒間の待機時間を設ける。
5.  **HTML解析:** `BeautifulSoup4`ライブラリでHTMLを解析する。
6.  **投稿単位での分割:** ページ内の各投稿（`<section class="recruit_content">`）をすべて取得し、ループ処理を行う。
7.  **データ抽出:** 各投稿のHTMLから、下記「4. データ抽出ロジック詳細」に基づき全データを抽出する。**データが取得できない項目があった場合でもプログラムが停止しないよう、`try-except`ブロックや`if`文で適切にエラーハンドリングを行う。**
8.  **データベース保存:** 抽出したデータを`posts`テーブルに `INSERT OR REPLACE` を使って保存する。

## 4. データ抽出ロジック詳細

各投稿のHTML要素（`section.recruit_content`）から、以下のCSSセレクタと抽出ロジックを用いてデータを取得します。

| 
データ項目

 | 

抽出元CSSセレクタ

 | 

抽出・加工ロジック

 |
| --- | --- | --- |
| 

**post\_id**

 | 

`h2 a[href^='/post/detail/']`

 | 

`href`属性値から正規表現 `r'/(\d+)$'` で末尾の数値を抽出。

 |
| 

**post\_datetime**

 | 

`span.list_created_time > time`

 | 

`.text`でテキストコンテンツを取得。

 |
| 

**title**

 | 

`h2[class*='purpose']`

 | 

`.text`で取得後、内部の`span`タグのテキストを除去し`strip()`で整形。

 |
| 

**purpose**

 | 

`.tag_sv > p[class*='purpose']`

 | 

`.text`で取得。

 |
| 

**original\_text**

 | 

`.pr_comment p`

 | 

`.text`で取得。

 |
| 

**author\_name**

 | 

`.profile p`

 | 

`.find('span').previous_sibling` で`<span>`タグ直前のテキストノードを取得。

 |
| 

**author\_real\_age**

 | 

`.profile p > span`

 | 

`.text`で取得後、正規表現 `r'(\S+代)'` で年代を抽出。なければ `？代`。

 |
| 

**author\_real\_gender**

 | 

`.profile p > span`

 | 

`.text`で取得後、正規表現 `r'(男性|女性)'` で性別を抽出。なければ `性別非公開`。

 |
| 

**author\_char\_race**

 | 

`.profile p`

 | 

`.find('br').next_sibling` のテキストを取得。例: `アウラ【アウラ・ ゼラ】`

 |
| 

**author\_char\_gender**

 | 

`.profile p > b`

 | 

`.text`で取得。

 |
| 

**author\_char\_job**

 | 

`.profile p`

 | 

`.text`で取得後、正規表現 `r'/\s*(.+)$'` で`/`以降の文字列を抽出。

 |
| 

**server**

 | 

`.tag_sv > p.server` (1番目)

 | 

1番目にヒットした要素の`.text`を取得。

 |
| 

**voice\_chat**

 | 

`dl.colmun_list dt:contains('ボイスチャット') + dd`

 | 

`.text`で取得。

 |
| 

**external\_tools**

 | 

`.profile dt:contains('外部ツール') + dd .choices_list`

 | 

`[tag.text for tag in elements]` でリスト化しJSON文字列に変換。

 |
| 

**playstyle\_tags**

 | 

`.tag_pr > p`

 | 

`[tag.text for tag in elements]` でリスト化しJSON文字列に変換。

 |
| 

**server\_transfer**

 | 

`.tag_sv > p.server`

 | 

`移転`を含むテキストを持つ要素を検索し格納。

 |
| 

**sub\_char\_ok**

 | 

`.tag_sv > p.sub`

 | 

要素が存在すれば `1`、存在しなければ `0` を格納。

 |
| 

**activity\_times**

 | 

`dl.colmun_list dt:contains('活動時間') + dd .choices_list`

 | 

`[tag.text for tag in elements]` でリスト化しJSON文字列に変換。

 |
| 

**wish\_races**

 | 

`dl.colmun_list dt:contains('希望種族') + dd .choices_list`

 | 

同上

 |
| 

**wish\_char\_genders**

 | 

`dl.colmun_list dt:contains('希望キャラ性別') + dd .choices_list`

 | 

同上

 |
| 

**wish\_jobs**

 | 

`dl.colmun_list dt:contains('希望ジョブ') + dd .choices_list`

 | 

同上

 |
| 

**wish\_real\_genders**

 | 

`dl.colmun_list dt:contains('希望リアル性別') + dd .choices_list`

 | 

同上

 |
| 

**wish\_real\_ages**

 | 

`dl.colmun_list dt:contains('希望リアル年代') + dd .choices_list`

 | 

同上

 |

## 5. 実装コードの骨子 (`scraper.py`)

このコードは、上記仕様を実装したスケルトンです。これをベースに開発を進めてください。

```
import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
import json
import logging

# ロギングの設定
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

BASE_URL = "[https://find-fr.com/](https://find-fr.com/)"
DB_NAME = "fap_posts.db"

def setup_database():
    """データベースとテーブルをセットアップする"""
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS posts (
        post_id INTEGER PRIMARY KEY, post_datetime TEXT, title TEXT, purpose TEXT, original_text TEXT,
        author_name TEXT, author_real_age TEXT, author_real_gender TEXT, author_char_race TEXT,
        author_char_gender TEXT, author_char_job TEXT, server TEXT, voice_chat TEXT,
        external_tools TEXT, playstyle_tags TEXT, server_transfer TEXT, sub_char_ok INTEGER,
        activity_times TEXT, wish_races TEXT, wish_char_genders TEXT, wish_jobs TEXT,
        wish_real_genders TEXT, wish_real_ages TEXT
    )
    """)
    conn.commit()
    return conn

def get_text_or_none(element, selector):
    """セレクタに一致する要素のテキストを返す。なければNone"""
    tag = element.select_one(selector)
    return tag.text.strip() if tag else None

def get_all_texts_as_json(element, selector):
    """セレクタに一致する全要素のテキストをリスト化し、JSON文字列で返す"""
    tags = element.select(selector)
    return json.dumps([tag.text.strip() for tag in tags], ensure_ascii=False) if tags else '[]'

def parse_post(post_section):
    """1つの投稿セクションから全てのデータを抽出し、辞書として返す"""
    data = {}
    try:
        # --- 基本情報 ---
        post_id_tag = post_section.select_one("h2 a[href^='/post/detail/']")
        if not post_id_tag: return None
        match = re.search(r'/(\d+)$', post_id_tag['href'])
        data['post_id'] = int(match.group(1)) if match else None

        data['post_datetime'] = get_text_or_none(post_section, "span.list_created_time > time")
        
        title_h2 = post_section.select_one("h2[class*='purpose']")
        if title_h2:
            time_span = title_h2.select_one("span.list_created_time")
            data['title'] = title_h2.text.replace(time_span.text, '').strip() if time_span else title_h2.text.strip()

        data['purpose'] = get_text_or_none(post_section, ".tag_sv > p[class*='purpose']")
        data['original_text'] = get_text_or_none(post_section, ".pr_comment p")

        # --- 募集主プロフィール ---
        profile_p = post_section.select_one(".profile p")
        if profile_p:
            span_tag = profile_p.select_one("span")
            if span_tag:
                data['author_name'] = span_tag.previous_sibling.strip() if span_tag.previous_sibling else ''
                real_profile_text = span_tag.text
                age_match = re.search(r'(\S+代)', real_profile_text)
                gender_match = re.search(r'(男性|女性)', real_profile_text)
                data['author_real_age'] = age_match.group(1) if age_match else '？代'
                data['author_real_gender'] = gender_match.group(1) if gender_match else '性別非公開'
            
            br_tag = profile_p.find('br')
            if br_tag and br_tag.next_sibling:
                full_race_text = br_tag.next_sibling.strip().split('/')[0].strip()
                data['author_char_race'] = full_race_text

            data['author_char_gender'] = get_text_or_none(profile_p, "b")

            job_match = re.search(r'/\s*(.+)$', profile_p.text)
            data['author_char_job'] = job_match.group(1).strip() if job_match else None

        # --- プレイ環境・スタイル ---
        server_tags = post_section.select(".tag_sv > p.server")
        data['server'] = server_tags[0].text.strip() if server_tags else None
        
        transfer_tag = [tag.text.strip() for tag in server_tags if "移転" in tag.text.strip()]
        data['server_transfer'] = transfer_tag[0] if transfer_tag else '移転不可' # デフォルト値
        
        data['sub_char_ok'] = 1 if post_section.select_one(".tag_sv > p.sub") else 0
        
        data['voice_chat'] = get_text_or_none(post_section, "dl.colmun_list dt:contains('ボイスチャット') + dd")
        data['external_tools'] = get_all_texts_as_json(post_section, ".profile dt:contains('外部ツール') + dd .choices_list")
        data['playstyle_tags'] = get_all_texts_as_json(post_section, ".tag_pr > p")

        # --- 希望条件 ---
        data['activity_times'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('活動時間') + dd .choices_list")
        data['wish_races'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('希望種族') + dd .choices_list")
        data['wish_char_genders'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('希望キャラ性別') + dd .choices_list")
        data['wish_jobs'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('希望ジョブ') + dd .choices_list")
        data['wish_real_genders'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('希望リアル性別') + dd .choices_list")
        data['wish_real_ages'] = get_all_texts_as_json(post_section, "dl.colmun_list dt:contains('希望リアル年代') + dd .choices_list")

        return data
    except Exception as e:
        logging.error(f"投稿ID {data.get('post_id', 'N/A')} の解析中にエラー: {e}", exc_info=True)
        return None

def save_to_db(data, conn):
    """抽出したデータをデータベースに保存する"""
    cursor = conn.cursor()
    valid_data = {k: v for k, v in data.items() if v is not None}
    columns = ', '.join(valid_data.keys())
    placeholders = ', '.join('?' * len(valid_data))
    sql = f"INSERT OR REPLACE INTO posts ({columns}) VALUES ({placeholders})"
    cursor.execute(sql, list(valid_data.values()))
    conn.commit()

def main(start_page=1, end_page=1):
    """メインのスクレイピング処理"""
    logging.info("スクレイピング処理を開始します。")
    conn = setup_database()
    
    for page_num in range(start_page, end_page + 1):
        target_url = f"{BASE_URL}?p={page_num}"
        logging.info(f"{page_num}ページ目を処理中: {target_url}")
        
        try:
            response = requests.get(target_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            posts = soup.select("section.recruit_content")
            if not posts:
                logging.warning(f"{page_num}ページ目に投稿なし。処理を終了します。")
                break
                
            for post_section in posts:
                post_data = parse_post(post_section)
                if post_data and post_data.get('post_id'):
                    save_to_db(post_data, conn)
            
            logging.info(f"{page_num}ページ目から{len(posts)}件の投稿を処理しました。")
        except requests.RequestException as e:
            logging.error(f"{page_num}ページ目の取得に失敗: {e}")
        
        time.sleep(5)

    conn.close()
    logging.info("スクレイピング処理が完了しました。")

if __name__ == "__main__":
    # --- 実行と検証の手順 ---
    # 1. まずは1ページだけでテスト実行
    main(start_page=1, end_page=1)

    # 2. DB Browser for SQLiteでfap_posts.dbを開き、データが正しく保存されているか確認
    
    # 3. 1ページのテストが成功したら、全ページを対象に実行
    # main(start_page=1, end_page=143)

```

## 6. 実行と検証の手順

ご提案いただいた通り、以下の2段階で実行と検証を行います。

### ステップ1: 1ページのみのテスト実行

1.  VSCodeのターミナルで、`scraper.py` を実行します。
    
    ```
    python scraper.py
    
    ```
    
2.  スクリプトは1ページ目のみをスクレイピングし、終了します。
3.  `DB Browser for SQLite` を起動し、「データベースを開く」からプロジェクトフォルダ内の `fap_posts.db` を選択します。
4.  「データ閲覧」タブで `posts` テーブルを選択し、データが仕様書通りに抽出・保存されているかを確認します。特に、新しく追加されたカラム（`purpose`, `wish_races`など）も含めて、データが意図通りに保存されているかを確認します。

### ステップ2: 全ページの実行

1.  ステップ1での検証が完了し、データの抽出に問題がないことを確認したら、`scraper.py` の末尾にある `main` 関数の呼び出し部分を以下のように変更します。
    
    ```
    if __name__ == "__main__":
        # 1ページのテストが成功したら、全ページを対象に実行
        main(start_page=1, end_page=143)
    
    ```
    
2.  再度、ターミナルで `scraper.py` を実行します。
3.  処理には `143ページ * 5秒/ページ ≒ 約12分` 程度の時間がかかります。ログに出力される進捗を見守ってください。
4.  処理完了後、再度 `DB Browser for SQLite` でデータベースを確認し、データが正しく蓄積されていることを確認します。