# 投稿評価アルゴリズム仕様書 (Ver. 5.3 "Orionis-Prime-Final")

## 1. 目的

本アルゴリズムは、FF14フレンド募集サイト「FaP」の投稿データに基づき、各投稿の「ユニーク性」を評価し、スコアリングすることを目的とする。

評価の最終目標は、**「サイトを初めて利用する真の新規ユーザー」または「投稿間隔が十分に空いたユーザー」を高く評価し、短期間での繰り返し投稿や、投稿間隔を意図的に空けている常習的な投稿者**の評価を厳格に抑制することにある。

この目的を達成するため、「Identity Triangulation」の概念に基づき、「静的プロファイル」「行動・嗜好パターン」「言語的指紋」の3側面から投稿者の特徴を捉え、人物の同一性を極めて高い精度で特定する。

## 2. 評価の基本フロー

1.  **Step 1: 高速フィルタリング (Candidate Pruning)**
2.  **Step 2: Identity Triangulationによる類似度分析**
3.  **Step 3: 偽装情報の補正と再投稿判定**
4.  **Step 4: 最終ユニークスコアの算出**

## 3. 抽出・生成する特徴データ

| **カテゴリ** | **データ項目** | **抽出元 CSSセレクタ/情報** | **データ形式** | **備考** |
| --- | --- | --- | --- | --- |
| 
**投稿基本情報**

 | 

投稿ID

 | 

`a[href^='/post/detail/']`

 | 

Integer

 | 

  


 |
| 

  


 | 

投稿日時

 | 

`span.list_created_time > time`

 | 

DateTime

 | 

  


 |
| 

  


 | 

投稿タイトル

 | 

`h2.purpose*`

 | 

String

 | 

  


 |
| 

**静的プロファイル**

 | 

**リアルプロファイル**

 | 

`.profile p > span` (正規表現)

 | 

Object `{age: int, gender: str}`

 | 

**【最重要】** 年代と性別を抽出

 |
| 

  


 | 

投稿者名

 | 

`.profile` p (正規表現で`<span>`タグの手前)

 | 

String

 | 

  


 |
| 

  


 | 

キャラクター種族

 | 

`.profile p` (正規表現で`<br>`の後の文字列)

 | 

String

 | 

  


 |
| 

  


 | 

キャラクター性別

 | 

`.profile p > b`

 | 

String

 | 

  


 |
| 

  


 | 

キャラクターJOB

 | 

`.profile p` (正規表現で`/`の後の文字列)

 | 

String

 | 

  


 |
| 

  


 | 

活動サーバー

 | 

`.tag_sv > .server`

 | 

String

 | 

  


 |
| 

**行動・嗜好パターン**

 | 

全タグ集合

 | 

`.tag_pr > p`, `.tag_sv > p` (目的・サーバー名を除く), `dt:contains('活動時間') + dd > .choices_list`, `dt:contains('希望')` + dd > `.choices_list`, `dt:contains('ボイスチャット') + dd`, `dt:contains('外部ツール') + dd > .choices_list`

 | 

Set<String>

 | 

存在する全てのタグを集合として格納。**移転の可否、サブキャラ可も含む**

 |
| 

**言語的指紋**

 | 

原文テキスト

 | 

`.pr_comment p`

 | 

String

 | 

**正規化せず、HTMLタグのみ除去して保存**

 |
| 

  


 | 

**正規化テキスト**

 | 

(原文テキストを加工)

 | 

String

 | 

意味内容分析用。詳細は下記参照

 |
| 

  


 | 

**意味内容ベクトル**

 | 

(正規化テキストをベクトル化)

 | 

Vector

 | 

意味内容の類似度計算用

 |
| 

  


 | 

**文体特徴ベクトル**

 | 

(原文テキストから算出)

 | 

Vector

 | 

文体のクセの類似度計算用

 |
| 

  


 | 

**サーバー偽装フラグ**

 | 

(原文テキストを正規表現で解析)

 | 

Boolean

 | 

本文中に偽装を示す単語があるか

 |

### 3.1. 言語的指紋の生成ロジック詳細

開発者が実装に迷わないよう、言語的指紋に関連する各特徴データの生成方法を以下に定義する。

#### A) 正規化テキスト (Normalized Text)

意味内容の類似度を純粋に比較するため、原文テキストから文体に関するノイズを除去する。

*   **処理手順:**
    
    1.  全ての英数字を半角に、カタカナを全角に統一する。
    2.  句読点、感嘆符、疑問符などの一般的な記号（`、。！？`）以外の記号、絵文字を全て除去する。
    3.  連続する空白や改行を全て半角スペース1つに置換する。
    4.  テキスト全体を小文字に変換する（英語が含まれる場合）。

#### B) 意味内容ベクトル (Semantic Vector)

*   **目的:** テキストが「何について書かれているか」を数値ベクトルで表現する。
*   **手法:** **TF-IDF (Term Frequency-Inverse Document Frequency)** を用いる。
    
    1.  形態素解析ライブラリを用いて、正規化テキストを単語（名詞、動詞、形容詞など）に分割する（わかち書き）。
    2.  ストップワード（「です」「ます」「の」など一般的すぎる単語）を除去する。
    3.  全投稿の単語から語彙リスト（コーパス）を作成し、各投稿をTF-IDFによってベクトル化する。

#### C) 文体特徴ベクトル (Stylistic Feature Vector)

*   **目的:** 文章の書き方の「クセ」を多角的に捉え、数値ベクトルで表現する。原文テキストから直接算出する。
*   **ベクトルの構成要素:**
    
    1.  **全角/半角使用比率:** 本文中の英数字と記号における `全角文字数 / (全角文字数 + 半角文字数)` の比率。 (Float)
    2.  **特殊記号・絵文字使用頻度:** 特定の記号や絵文字（例: `！,？,w,♪,✨,🍀,🎀,🐰,🧸,💌`など）の出現回数を本文文字数で割った値。 (Float)
    3.  **句読点後スペース率:** 全ての句読点（`。` `、`）のうち、直後にスペースが存在する割合。 (Float)
    4.  **改行頻度:** `改行文字数 / 本文文字数`。ただし、本文文字数が0の場合は0とする。 (Float)
    5.  **ひらがな比率:** `ひらがなの文字数 / 本文文字数`。 (Float)
    6.  **カタカナ比率:** `カタカナの文字数 / 本文文字数`。 (Float)
    7.  **漢字比率:** `漢字の文字数 / 本文文字数`。 (Float)
    8.  **難読漢字使用率:** `常用漢字外の漢字数 / 本文文字数`。常用漢字リストを別途用意し、それに含まれない漢字をカウントする。 (Float)
    9.  **特定単語のひらがな/漢字使用傾向:** 特定の単語（例: `ありがとう`, `よろしく`, `ください`）が、ひらがなで書かれているか漢字で書かれているかを判定し、その傾向をベクトル化する（例: `{"ありがとう": 1, "宜しく": 0, "下さい": 0}` のように0/1で表現）。 (Vector/Dict)

## 4. スコアリングアルゴリズム (高精度仕様 "Orionis-Prime-Final")

### ステップ1：高速フィルタリング (DBクエリ)

1.  新規投稿から「**リアル性別**」「**リアル年代**」「**キャラクター種族**」「**キャラクター性別**」の4つの静的情報を取得する。
2.  データベース（SQLite）に対し、上記4つのカラムが**完全に一致**する過去の投稿を検索候補とする。
3.  **【必須】** 上記4カラムには必ず複合インデックスを設定すること。

### ステップ2：Identity Triangulationによる類似度分析 (100点満点)

#### A) 静的プロファイル類似度スコア (45点満点)

*   **リアルプロファイル (35点):** `35 * (1 - min(5, |年代A - 年代B|) / 5)`
*   **キャラ情報 (5点):** キャラクター種族・性別が一致で5点。
*   **変動プロファイル (5点):** キャラクターJOB・投稿者名が一致で5点。

#### B) 行動・嗜好パターン類似度スコア (25点満点)

*   **手法:** 希少なタグの一致を重視する\*\*「重み付きJaccard係数」\*\*を用いる。
*   **タグの重み:** `w(t) = 1 / log(1 + (タグtの全投稿における出現回数))`
*   **スコア:** `25 * (Σ_{t∈A∩B} w(t)) / (Σ_{t∈A∪B} w(t))`

#### C) 言語的指紋類似度スコア (30点満点)

1.  **意味内容の類似度 (15点):** 新旧投稿の**意味内容ベクトル**間の**コサイン類似度**を計算。 `スコア = 15 * CosineSimilarity`
2.  **文体指紋の類似度 (15点):** 新旧投稿の**文体特徴ベクトル**間の**正規化ユークリッド距離**から類似度を算出。
3.  **信頼度係数の適用:**
    
    *   `信頼度係数 = min(1.0, 本文文字数 / 100)`
    *   `最終言語スコア` = (意味内容スコア + 文体指紋スコア) \* `信頼度係数`

### ステップ3：偽装情報の補正と再投稿判定

1.  **基礎類似度スコアの算出:** `A) + B) + C)`
2.  **自己申告による偽装情報の検出:** 本文を `/(サーバー|鯖).*?(フェイク|偽装|ダミー)/i` 等のパターンで検索し、「サーバー偽装フラグ」を設定。
3.  **一貫性ボーナスの適用:** 両投稿の偽装フラグが `False` の場合のみ、活動サーバーの一致で `+5点` のボーナス。
4.  最も高いスコアを**最大類似度スコア**とする。
5.  **不一致ボーナス（新規ユーザー救済措置）:**
    
    *   **目的:** 他人の投稿を参考にした結果、偶然スコアが高くなってしまった新規ユーザーを救済する。
    *   **条件:** `最大類似度スコア >= 88点` の投稿が見つかった場合にのみ、以下の追加チェックを行う。
        
        *   `IF (意味内容の類似度スコア >= 13)` _\-- 内容は酷似している_
        *   `AND (文体指紋の類似度スコア <= 7)` _\-- しかし、書き方のクセは全く違う_
    *   **処理:** 上記条件を満たす場合、`最大類似度スコア = 最大類似度スコア - 5` とする。
6.  **高確度再投稿の判定:** 最終的な `最大類似度スコア >= 88点` **AND** `静的プロファイルスコア >= 35点` の場合に判定。

### ステップ4：最終ユニークスコアの算出

1.  **再投稿ペナルティの計算:**
    
    *   「高確度再投稿」判定時、過去の同判定回数(N)と最終投稿からの経過日数(D)で算出。
    *   **基本ペナルティ:** `-30 - (15 * (N-1))`
    *   **回復上限:** `max(5, 15 - (N-2))`
    *   **時間回復ボーナス:** `min(回復上限, (D / 10) * 回復上限)`
    *   **再投稿ペナルティ:** `基本ペナルティ + 時間回復ボーナス`
2.  **最終スコアの算出:**
    
    *   `暫定スコア = 100 - 最大類似度スコア + 再投稿ペナルティ`
    *   `最終ユニークスコア = max(0, min(100, 暫定スコア))`

## 付録A: 実装のための推奨ライブラリ (Python)

本アルゴリズムの言語的指紋分析は、**「① テキストを単語に分割する」** → **「② 単語の羅列を数値（ベクトル）に変換する」** という2つの主要なステップで構成される。それぞれのステップで以下のライブラリの利用を推奨する。

### 1. 形態素解析器 (テキストの単語分割)

#### 根本的な役割

コンピュータは、人間のように「すもももももももものうち」という文章を直感的に単語に区切ることができません。形態素解析器の役割は、この連続した文字列を、意味を持つ最小単位（＝形態素、≒単語）に正確に分割することです。

**例:** `「高難易度もやります」` → `['高難易度', 'も', 'やり', 'ます']`

この分割処理は、コンピュータが文章の「意味」を理解するための、**最も重要かつ不可欠な第一歩**です。

#### 推奨: `MeCab` (`mecab-python3`)

*   **特徴:** 高速・高精度なオープンソースの形態素解析エンジン。C++で開発されており、動作が非常に軽い。多くの実績があり、本番環境での利用に適している。
*   **長所:** 処理速度が速く、サーバー負荷が低い。IPA辞書やNEologdなど、用途に応じた辞書を選択・追加することで解析精度を向上させられる。
*   **注意点:** C++本体と辞書のインストールが別途必要。環境構築に一手間かかる場合がある。

#### 代替案: `Janome`

*   **特徴:** Pure Pythonで実装された、インストールが容易な形態素解析ライブラリ。
*   **長所:** `pip install janome` だけで利用可能。環境構築が非常に簡単で、プロトタイピングや小規模なアプリケーションに最適。
*   **注意点:** MeCabに比べると処理速度が遅いため、大量のテキストをリアルタイムで処理する場合には性能がボトルネックになる可能性がある。

### 2. ベクトル化ライブラリ

#### 根本的な役割

形態素解析によって単語に分割した後、その単語の羅列をコンピュータが計算できる形式、すなわち\*\*数値のリスト（ベクトル）\*\*に変換する必要があります。このプロセスを「ベクトル化」と呼びます。

**例:** 語彙リストが `['高難易度', '零式', 'SS', '好き']` の場合

*   投稿A `「零式好きです」` → `[0, 1, 0, 1]`
*   投稿B `「SS撮影が好き」` → `[0, 0, 1, 1]`

一度ベクトルに変換してしまえば、投稿Aと投稿Bがどれだけ似ているかを、数学的な計算（コサイン類似度など）で客観的に評価できるようになります。

#### 推奨: `scikit-learn`

*   **特徴:** Pythonで最も標準的な機械学習ライブラリ。データの前処理からモデル構築、評価までを網羅した豊富な機能を持つ。
*   **長所:** `TfidfVectorizer` クラスが極めて強力。単語の出現回数を数えるだけでなく、「多くの投稿で使われる一般的な単語（例: 好き）の重みを下げ、特定の投稿にしか現れない珍しい単語（例: 絶オメガ）の重みを上げる」という**TF-IDF**の考え方に基づき、より賢くベクトル化を行ってくれる。コーパス作成からベクトル化までを一貫して効率的に実行できるため、本アルゴリズムの実装に最適。
*   **注意点:** 特になし。本アルゴリズムの実装において最適な選択肢。